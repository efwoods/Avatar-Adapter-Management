# Bug: This endpoint will not startup:

ase) linux-pc@pc:~/gh/projects/NeuralNexus/Avatar-Adapter-Management$ docker compose up --build
[+] Building 10.7s (15/15) FINISHED                                                             
 => [internal] load local bake definitions                                                 0.0s
 => => reading from stdin 629B                                                             0.0s
 => [internal] load build definition from Dockerfile.dev                                   0.0s
 => => transferring dockerfile: 555B                                                       0.0s
 => [internal] load metadata for docker.io/library/python:3.11-slim                        0.4s
 => [auth] library/python:pull token for registry-1.docker.io                              0.0s
 => [internal] load .dockerignore                                                          0.0s
 => => transferring context: 2B                                                            0.0s
 => [1/6] FROM docker.io/library/python:3.11-slim@sha256:a0939570b38cddeb861b8e75d20b1c82  0.0s
 => => resolve docker.io/library/python:3.11-slim@sha256:a0939570b38cddeb861b8e75d20b1c82  0.0s
 => [internal] load build context                                                          0.0s
 => => transferring context: 38B                                                           0.0s
 => CACHED [2/6] WORKDIR /app                                                              0.0s
 => CACHED [3/6] RUN apt-get update && apt-get install -y     build-essential     curl     0.0s
 => CACHED [4/6] COPY requirements.txt .                                                   0.0s
 => CACHED [5/6] RUN pip install --no-cache-dir -r requirements.txt                        0.0s
 => CACHED [6/6] RUN mkdir -p /app/temp_training_output                                    0.0s
 => exporting to docker image format                                                      10.2s
 => => exporting layers                                                                    0.0s
 => => exporting manifest sha256:62756adfddafa3cb86acdd71ea57069b7cb660520596f6732ba912fc  0.0s
 => => exporting config sha256:22dbbc740e416409866bd083a2a02c9fb1e25bcada7056ca998de130d9  0.0s
 => => sending tarball                                                                    10.2s
 => importing to docker                                                                    0.2s
 => resolving provenance for metadata file                                                 0.0s
[+] Running 2/2
 âœ” nn-avatar-adapter-management            Built                                           0.0s 
 âœ” Container nn-avatar-adapter-management  Run...                                          0.0s 
Attaching to nn-avatar-adapter-management

-----

# FastAPI LoRA Adapter Management System
## app/main.py
from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import RedirectResponse, Response
from contextlib import asynccontextmanager
import uvicorn
import boto3

from core.config import settings

# Import your existing routers
from api import adapters, training_data, persistence
from core.logging import logger
from service.persistence_service import (
    s3_client_instance, 
    get_s3_client, 
)


@asynccontextmanager
async def lifespan(app: FastAPI):
    global s3_client_instance
    
    # Startup
    logger.info("Application startup - Initializing S3 and settings")
    
    # Get settings and validate required fields
    # settings are initialized on startup

    user_id = settings.USER_ID
    if not user_id:
        logger.error("USER_ID setting is required")
        raise RuntimeError("USER_ID setting is required")
    
    logger.info(f"Starting Adapter Management API for user: {user_id}")
    logger.info(f"App: LoRA Adapter Management API v1.0.0")
    
    # Validate required settings
    if not settings.s3_bucket_name:
        logger.error("S3_BUCKET_NAME is required")
        raise RuntimeError("S3_BUCKET_NAME environment variable is required")
    
    # Initialize S3 client with error handling
    try:
        s3_client = boto3.client(
            's3',
            aws_access_key_id=settings.aws_access_key_id,
            aws_secret_access_key=settings.aws_secret_access_key,
            region_name=settings.aws_region
        )
        # Test S3 connection
        s3_client.head_bucket(Bucket=settings.s3_bucket_name)
        logger.info(f"S3 connection successful to bucket: {settings.s3_bucket_name}")
        s3_client_instance = s3_client
    except Exception as e:
        logger.error(f"Failed to initialize S3 client: {e}")
        raise RuntimeError(f"S3 initialization failed: {e}")
    
    logger.info(f"Adapter persistence configured for user: {user_id}")
    logger.info(f"S3 bucket: {settings.s3_bucket_name}")
    
    yield
    
    # Shutdown
    logger.info("Application shutdown - Cleaning up resources")
    s3_client_instance = None
    logger.info("Cleanup completed")

app = FastAPI(
    title="LoRA Adapter Management API",
    description="API for managing LoRA adapters with S3 storage",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(adapters.router, prefix="/adapters", tags=["adapters"])
app.include_router(training_data.router, prefix="/training-data", tags=["training-data"])
app.include_router(persistence.router, prefix="/persistence", tags=["persistence"])

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    try:
        s3_client = get_s3_client()
        
        # Test S3 connection
        s3_status = "connected"
        try:
            s3_client.head_bucket(Bucket=settings.s3_bucket_name)
        except Exception:
            s3_status = "disconnected"
        
        return {
            "status": "healthy",
            "s3_status": s3_status,
            "s3_bucket": settings.s3_bucket_name,
            "user_id": settings.USER_ID
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e)
        }

@app.get("/", tags=["ğŸ“– Documentation"])
async def root(request: Request):
    return RedirectResponse(url=f"{request.scope.get('root_path', '')}/docs")
----


# classes/AdapterPersistenceManager.py

from core.logging import logger
import tempfile
import zipfile
from typing import Dict, Any, List
from datetime import datetime
import json

from fastapi import HTTPException
from botocore.exceptions import ClientError
import os
class AdapterPersistenceManager:
    """Manages persistence operations for LoRA adapters and training data"""
    
    def __init__(self, s3_client, settings, user_id: str, avatar_id: str):
        self.s3_client = s3_client
        self.settings = settings
        self.user_id = user_id
        self.avatar_id = avatar_id
        self.s3_bucket = settings.s3_bucket_name
        
    def _get_s3_adapter_path(self) -> str:
        """Get S3 path for adapters"""
        return f"users/{self.user_id}/avatars/{self.avatar_id}/adapters/"
    
    def _get_s3_training_data_path(self) -> str:
        """Get S3 path for training data"""
        return f"users/{self.user_id}/avatars/{self.avatar_id}/adapters/training_data/"
    
    def _get_s3_metadata_path(self) -> str:
        """Get S3 path for adapter metadata"""
        return f"users/{self.user_id}/avatars/{self.avatar_id}/adapters/metadata/"
    
    async def backup_adapters_to_s3(self, local_adapter_path: str) -> Dict[str, Any]:
        """Backup adapter files to S3"""
        try:
            if not os.path.exists(local_adapter_path):
                raise HTTPException(
                    status_code=404,
                    detail=f"Local adapter path not found: {local_adapter_path}"
                )
            
            # Create zip file of adapters
            with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as temp_file:
                with zipfile.ZipFile(temp_file.name, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for root, dirs, files in os.walk(local_adapter_path):
                        for file in files:
                            file_path = os.path.join(root, file)
                            arcname = os.path.relpath(file_path, local_adapter_path)
                            zipf.write(file_path, arcname)
                
                # Upload to S3
                s3_key = f"{self._get_s3_adapter_path()}adapter_backup.zip"
                self.s3_client.upload_file(temp_file.name, self.s3_bucket, s3_key)
                
                # Create metadata
                metadata = {
                    "backup_type": "adapters",
                    "user_id": self.user_id,
                    "avatar_id": self.avatar_id,
                    "backup_timestamp": datetime.now().isoformat(),
                    "file_count": sum([len(files) for _, _, files in os.walk(local_adapter_path)]),
                    "backup_size_bytes": os.path.getsize(temp_file.name)
                }
                
                # Upload metadata
                metadata_key = f"{self._get_s3_adapter_path()}backup_metadata.json"
                self.s3_client.put_object(
                    Bucket=self.s3_bucket,
                    Key=metadata_key,
                    Body=json.dumps(metadata, indent=2),
                    ContentType='application/json'
                )
                
                # Cleanup temp file
                os.unlink(temp_file.name)
                
                return metadata
                
        except Exception as e:
            logger.error(f"Error backing up adapters: {e}")
            raise
    
    async def backup_training_data_to_s3(self, local_training_data_path: str) -> Dict[str, Any]:
        """Backup training data to S3"""
        try:
            if not os.path.exists(local_training_data_path):
                raise HTTPException(
                    status_code=404,
                    detail=f"Local training data path not found: {local_training_data_path}"
                )
            
            # Create zip file of training data
            with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as temp_file:
                with zipfile.ZipFile(temp_file.name, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for root, dirs, files in os.walk(local_training_data_path):
                        for file in files:
                            file_path = os.path.join(root, file)
                            arcname = os.path.relpath(file_path, local_training_data_path)
                            zipf.write(file_path, arcname)
                
                # Upload to S3
                s3_key = f"{self._get_s3_training_data_path()}training_data_backup.zip"
                self.s3_client.upload_file(temp_file.name, self.s3_bucket, s3_key)
                
                # Create metadata
                metadata = {
                    "backup_type": "training_data",
                    "user_id": self.user_id,
                    "avatar_id": self.avatar_id,
                    "backup_timestamp": datetime.now().isoformat(),
                    "file_count": sum([len(files) for _, _, files in os.walk(local_training_data_path)]),
                    "backup_size_bytes": os.path.getsize(temp_file.name)
                }
                
                # Upload metadata
                metadata_key = f"{self._get_s3_training_data_path()}backup_metadata.json"
                self.s3_client.put_object(
                    Bucket=self.s3_bucket,
                    Key=metadata_key,
                    Body=json.dumps(metadata, indent=2),
                    ContentType='application/json'
                )
                
                # Cleanup temp file
                os.unlink(temp_file.name)
                
                return metadata
                
        except Exception as e:
            logger.error(f"Error backing up training data: {e}")
            raise
    
    async def restore_adapters_from_s3(self, local_adapter_path: str) -> None:
        """Restore adapter files from S3"""
        try:
            s3_key = f"{self._get_s3_adapter_path()}adapter_backup.zip"
            
            # Check if backup exists
            try:
                self.s3_client.head_object(Bucket=self.s3_bucket, Key=s3_key)
            except ClientError as e:
                if e.response['Error']['Code'] == '404':
                    raise HTTPException(
                        status_code=404,
                        detail=f"No adapter backup found for user {self.user_id}, avatar {self.avatar_id}"
                    )
                raise
            
            # Create local directory if it doesn't exist
            os.makedirs(local_adapter_path, exist_ok=True)
            
            # Download and extract
            with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as temp_file:
                self.s3_client.download_file(self.s3_bucket, s3_key, temp_file.name)
                
                with zipfile.ZipFile(temp_file.name, 'r') as zipf:
                    zipf.extractall(local_adapter_path)
                
                # Cleanup temp file
                os.unlink(temp_file.name)
                
        except Exception as e:
            logger.error(f"Error restoring adapters: {e}")
            raise
    
    async def restore_training_data_from_s3(self, local_training_data_path: str) -> None:
        """Restore training data from S3"""
        try:
            s3_key = f"{self._get_s3_training_data_path()}training_data_backup.zip"
            
            # Check if backup exists
            try:
                self.s3_client.head_object(Bucket=self.s3_bucket, Key=s3_key)
            except ClientError as e:
                if e.response['Error']['Code'] == '404':
                    raise HTTPException(
                        status_code=404,
                        detail=f"No training data backup found for user {self.user_id}, avatar {self.avatar_id}"
                    )
                raise
            
            # Create local directory if it doesn't exist
            os.makedirs(local_training_data_path, exist_ok=True)
            
            # Download and extract
            with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as temp_file:
                self.s3_client.download_file(self.s3_bucket, s3_key, temp_file.name)
                
                with zipfile.ZipFile(temp_file.name, 'r') as zipf:
                    zipf.extractall(local_training_data_path)
                
                # Cleanup temp file
                os.unlink(temp_file.name)
                
        except Exception as e:
            logger.error(f"Error restoring training data: {e}")
            raise
    
    async def list_adapter_backups(self) -> List[Dict[str, Any]]:
        """List available adapter backups"""
        backups = []
        
        try:
            # List adapter backups
            adapter_prefix = self._get_s3_adapter_path()
            response = self.s3_client.list_objects_v2(
                Bucket=self.s3_bucket,
                Prefix=adapter_prefix
            )
            
            if 'Contents' in response:
                for obj in response['Contents']:
                    if obj['Key'].endswith('adapter_backup.zip'):
                        # Try to get metadata
                        metadata_key = obj['Key'].replace('adapter_backup.zip', 'backup_metadata.json')
                        metadata = {}
                        try:
                            metadata_obj = self.s3_client.get_object(Bucket=self.s3_bucket, Key=metadata_key)
                            metadata = json.loads(metadata_obj['Body'].read().decode('utf-8'))
                        except:
                            pass
                        
                        backups.append({
                            "type": "adapters",
                            "key": obj['Key'],
                            "size": obj['Size'],
                            "last_modified": obj['LastModified'].isoformat(),
                            "metadata": metadata
                        })
            
            # List training data backups
            training_prefix = self._get_s3_training_data_path()
            response = self.s3_client.list_objects_v2(
                Bucket=self.s3_bucket,
                Prefix=training_prefix
            )
            
            if 'Contents' in response:
                for obj in response['Contents']:
                    if obj['Key'].endswith('training_data_backup.zip'):
                        # Try to get metadata
                        metadata_key = obj['Key'].replace('training_data_backup.zip', 'backup_metadata.json')
                        metadata = {}
                        try:
                            metadata_obj = self.s3_client.get_object(Bucket=self.s3_bucket, Key=metadata_key)
                            metadata = json.loads(metadata_obj['Body'].read().decode('utf-8'))
                        except:
                            pass
                        
                        backups.append({
                            "type": "training_data",
                            "key": obj['Key'],
                            "size": obj['Size'],
                            "last_modified": obj['LastModified'].isoformat(),
                            "metadata": metadata
                        })
            
            return backups
            
        except Exception as e:
            logger.error(f"Error listing adapter backups: {e}")
            raise

---

from fastapi import APIRouter, HTTPException, Depends
from fastapi.responses import FileResponse
from typing import Optional, Dict, Any
import os
import tempfile
import json
from datetime import datetime

from classes.lora_manager import LoRAManager
from db.schema.models import TrainingRequest, AdapterConfig
from service.persistence_service import get_adapter_persistence_manager
from service.training_service import TrainingService
from service.s3_service import S3Service
from core.logging import logger

router = APIRouter()
lora_manager = LoRAManager()
training_service = TrainingService()
s3_service = S3Service()

@router.post("/{user_id}/{avatar_id}/create")
async def create_adapter(
    user_id: str, 
    avatar_id: str, 
    adapter_name: str = "default"
) -> AdapterConfig:
    """Create a new adapter configuration"""
    try:
        # Get persistence manager
        persistence_manager = get_adapter_persistence_manager(avatar_id)
        
        # Check if adapter already exists
        adapter_path = f"users/{user_id}/avatars/{avatar_id}/adapters/"
        
        try:
            # Try to list existing adapters
            response = persistence_manager.s3_client.list_objects_v2(
                Bucket=persistence_manager.s3_bucket,
                Prefix=adapter_path
            )
            
            if 'Contents' in response and any(obj['Key'].endswith('adapter_backup.zip') for obj in response['Contents']):
                logger.info(f"Adapter already exists for user {user_id}, avatar {avatar_id}")
                # Return existing adapter config
                return AdapterConfig(
                    user_id=user_id,
                    avatar_id=avatar_id,
                    adapter_name=adapter_name,
                    status="existing",
                    created_at=datetime.now(),
                    s3_path=adapter_path
                )
        except Exception as e:
            logger.warning(f"Error checking existing adapter: {e}")
        
        # Create new adapter locally
        with tempfile.TemporaryDirectory() as temp_dir:
            local_adapter_path = os.path.join(temp_dir, "adapters")
            os.makedirs(local_adapter_path, exist_ok=True)
            
            # Initialize empty adapter structure
            adapter_config = {
                "adapter_name": adapter_name,
                "user_id": user_id,
                "avatar_id": avatar_id,
                "created_at": datetime.now().isoformat(),
                "version": "1.0.0",
                "status": "untrained",
                "training_history": []
            }
            
            # Save adapter config
            config_path = os.path.join(local_adapter_path, "adapter_config.json")
            with open(config_path, 'w') as f:
                json.dump(adapter_config, f, indent=2)
            
            # Create placeholder adapter files (LoRA specific structure)
            lora_structure = {
                "adapter_model.bin": b"",  # Placeholder for actual adapter weights
                "adapter_config.json": json.dumps({
                    "target_modules": ["q_proj", "v_proj"],
                    "r": 16,
                    "lora_alpha": 32,
                    "lora_dropout": 0.1
                }).encode()
            }
            
            for filename, content in lora_structure.items():
                file_path = os.path.join(local_adapter_path, filename)
                with open(file_path, 'wb') as f:
                    f.write(content)
            
            # Backup to S3
            backup_metadata = await persistence_manager.backup_adapters_to_s3(local_adapter_path)
            
            logger.info(f"Created and backed up new adapter for user {user_id}, avatar {avatar_id}")
            
            return AdapterConfig(
                user_id=user_id,
                avatar_id=avatar_id,
                adapter_name=adapter_name,
                status="created",
                created_at=datetime.now(),
                s3_path=adapter_path,
                metadata=backup_metadata
            )
            
    except Exception as e:
        logger.error(f"Error creating adapter: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to create adapter: {str(e)}")

@router.post("/{user_id}/{avatar_id}/train")
async def train_adapter(
    user_id: str, 
    avatar_id: str, 
    training_params: Optional[Dict] = None
):
    """Train a LoRA adapter"""
    try:
        # Get persistence manager
        persistence_manager = get_adapter_persistence_manager(avatar_id)
        
        # Get metadata to determine which files to use for training
        metadata_path = f"users/{user_id}/avatars/{avatar_id}/adapters/metadata/"
        metadata_key = f"{metadata_path}metadata.json"
        
        training_files = []
        try:
            # Get training metadata
            metadata_obj = persistence_manager.s3_client.get_object(
                Bucket=persistence_manager.s3_bucket,
                Key=metadata_key
            )
            metadata = json.loads(metadata_obj['Body'].read().decode('utf-8'))
            
            # Filter files marked for training
            training_files = [filename for filename, use_for_training in metadata.items() if use_for_training]
            logger.info(f"Found {len(training_files)} files marked for training")
            
        except Exception as e:
            logger.warning(f"No training metadata found: {e}. Proceeding without training data.")
        
        with tempfile.TemporaryDirectory() as temp_dir:
            local_adapter_path = os.path.join(temp_dir, "adapters")
            local_training_path = os.path.join(temp_dir, "training_data")
            os.makedirs(local_adapter_path, exist_ok=True)
            os.makedirs(local_training_path, exist_ok=True)
            
            # Restore existing adapter or create new one
            try:
                await persistence_manager.restore_adapters_from_s3(local_adapter_path)
                logger.info("Restored existing adapter from S3")
            except HTTPException as e:
                if e.status_code == 404:
                    # Create new adapter if none exists
                    logger.info("No existing adapter found, creating new one")
                    await create_adapter(user_id, avatar_id)
                    await persistence_manager.restore_adapters_from_s3(local_adapter_path)
                else:
                    raise
            
            # Download training data if available
            if training_files:
                training_data_path = f"users/{user_id}/avatars/{avatar_id}/adapters/training_data/"
                
                for filename in training_files:
                    try:
                        file_key = f"{training_data_path}{filename}"
                        local_file_path = os.path.join(local_training_path, filename)
                        
                        persistence_manager.s3_client.download_file(
                            persistence_manager.s3_bucket,
                            file_key,
                            local_file_path
                        )
                        logger.info(f"Downloaded training file: {filename}")
                    except Exception as e:
                        logger.warning(f"Failed to download training file {filename}: {e}")
            
            # Train the adapter
            if training_files and os.listdir(local_training_path):
                try:
                    # Use training service to train the adapter
                    training_result = await training_service.train_lora_adapter(
                        adapter_path=local_adapter_path,
                        training_data_path=local_training_path,
                        training_params=training_params or {}
                    )
                    
                    # Update adapter config with training info
                    config_path = os.path.join(local_adapter_path, "adapter_config.json")
                    if os.path.exists(config_path):
                        with open(config_path, 'r') as f:
                            adapter_config = json.load(f)
                        
                        adapter_config.update({
                            "status": "trained",
                            "last_training": datetime.now().isoformat(),
                            "training_files_used": training_files,
                            "training_result": training_result
                        })
                        
                        # Add to training history
                        if "training_history" not in adapter_config:
                            adapter_config["training_history"] = []
                        
                        adapter_config["training_history"].append({
                            "timestamp": datetime.now().isoformat(),
                            "files_used": training_files,
                            "training_params": training_params,
                            "result": training_result
                        })
                        
                        with open(config_path, 'w') as f:
                            json.dump(adapter_config, f, indent=2)
                    
                    logger.info(f"Successfully trained adapter for user {user_id}, avatar {avatar_id}")
                    
                except Exception as e:
                    logger.error(f"Training failed: {e}")
                    # Update status to indicate training failure
                    config_path = os.path.join(local_adapter_path, "adapter_config.json")
                    if os.path.exists(config_path):
                        with open(config_path, 'r') as f:
                            adapter_config = json.load(f)
                        adapter_config["status"] = "training_failed"
                        adapter_config["last_error"] = str(e)
                        with open(config_path, 'w') as f:
                            json.dump(adapter_config, f, indent=2)
            else:
                logger.info("No training data available, uploading untrained adapter")
                # Update status to untrained
                config_path = os.path.join(local_adapter_path, "adapter_config.json")
                if os.path.exists(config_path):
                    with open(config_path, 'r') as f:
                        adapter_config = json.load(f)
                    adapter_config["status"] = "untrained"
                    adapter_config["last_updated"] = datetime.now().isoformat()
                    with open(config_path, 'w') as f:
                        json.dump(adapter_config, f, indent=2)
            
            # Backup trained/updated adapter to S3
            backup_metadata = await persistence_manager.backup_adapters_to_s3(local_adapter_path)
            
            return {
                "status": "success",
                "message": f"Adapter training completed for user {user_id}, avatar {avatar_id}",
                "training_files_used": training_files,
                "backup_metadata": backup_metadata
            }
            
    except Exception as e:
        logger.error(f"Error training adapter: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to train adapter: {str(e)}")

@router.delete("/{user_id}/{avatar_id}")
async def delete_adapter(user_id: str, avatar_id: str):
    """Delete an adapter"""
    try:
        # Get persistence manager
        persistence_manager = get_adapter_persistence_manager(avatar_id)
        
        # Define S3 paths to delete
        adapter_path = f"users/{user_id}/avatars/{avatar_id}/adapters/"
        
        # List all objects with the adapter prefix
        response = persistence_manager.s3_client.list_objects_v2(
            Bucket=persistence_manager.s3_bucket,
            Prefix=adapter_path
        )
        
        if 'Contents' not in response:
            raise HTTPException(
                status_code=404,
                detail=f"No adapter found for user {user_id}, avatar {avatar_id}"
            )
        
        # Delete all adapter-related objects
        objects_to_delete = [{'Key': obj['Key']} for obj in response['Contents']]
        
        if objects_to_delete:
            persistence_manager.s3_client.delete_objects(
                Bucket=persistence_manager.s3_bucket,
                Delete={'Objects': objects_to_delete}
            )
            
            logger.info(f"Deleted {len(objects_to_delete)} adapter objects for user {user_id}, avatar {avatar_id}")
        
        return {
            "status": "success",
            "message": f"Adapter deleted for user {user_id}, avatar {avatar_id}",
            "deleted_objects": len(objects_to_delete)
        }
        
    except Exception as e:
        logger.error(f"Error deleting adapter: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to delete adapter: {str(e)}")

@router.get("/{user_id}/{avatar_id}")
async def get_adapter(user_id: str, avatar_id: str):
    """Get an adapter - returns adapter file for download or creates new one if doesn't exist"""
    try:
        # Get persistence manager
        persistence_manager = get_adapter_persistence_manager(avatar_id)
        
        # Check if adapter exists
        adapter_path = f"users/{user_id}/avatars/{avatar_id}/adapters/"
        adapter_key = f"{adapter_path}adapter_backup.zip"
        
        try:
            # Check if adapter exists in S3
            persistence_manager.s3_client.head_object(
                Bucket=persistence_manager.s3_bucket,
                Key=adapter_key
            )
            
            # Adapter exists, download and return
            with tempfile.NamedTemporaryFile(delete=False, suffix='.zip') as temp_file:
                persistence_manager.s3_client.download_file(
                    persistence_manager.s3_bucket,
                    adapter_key,
                    temp_file.name
                )
                
                # Get adapter metadata
                try:
                    metadata_key = f"{adapter_path}backup_metadata.json"
                    metadata_obj = persistence_manager.s3_client.get_object(
                        Bucket=persistence_manager.s3_bucket,
                        Key=metadata_key
                    )
                    metadata = json.loads(metadata_obj['Body'].read().decode('utf-8'))
                except:
                    metadata = {}
                
                logger.info(f"Retrieved existing adapter for user {user_id}, avatar {avatar_id}")
                
                # Return file response for download
                return FileResponse(
                    path=temp_file.name,
                    filename=f"adapter_{user_id}_{avatar_id}.zip",
                    media_type="application/zip",
                    headers={
                        "X-Adapter-Metadata": json.dumps(metadata),
                        "X-User-ID": user_id,
                        "X-Avatar-ID": avatar_id
                    }
                )
                
        except Exception as e:
            if "404" in str(e) or "Not Found" in str(e):
                # Adapter doesn't exist, create new one
                logger.info(f"No existing adapter found for user {user_id}, avatar {avatar_id}, creating new one")
                
                # Create new adapter
                adapter_config = await create_adapter(user_id, avatar_id)
                
                # Now retrieve the newly created adapter
                with tempfile.NamedTemporaryFile(delete=False, suffix='.zip') as temp_file:
                    persistence_manager.s3_client.download_file(
                        persistence_manager.s3_bucket,
                        adapter_key,
                        temp_file.name
                    )
                    
                    return FileResponse(
                        path=temp_file.name,
                        filename=f"adapter_{user_id}_{avatar_id}.zip",
                        media_type="application/zip",
                        headers={
                            "X-Adapter-Status": "newly_created",
                            "X-User-ID": user_id,
                            "X-Avatar-ID": avatar_id
                        }
                    )
            else:
                raise
        
    except Exception as e:
        logger.error(f"Error getting adapter: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get adapter: {str(e)}")

@router.get("/{user_id}/{avatar_id}/info")
async def get_adapter_info(user_id: str, avatar_id: str):
    """Get adapter information without downloading the file"""
    try:
        # Get persistence manager
        persistence_manager = get_adapter_persistence_manager(avatar_id)
        
        # Get adapter metadata
        adapter_path = f"users/{user_id}/avatars/{avatar_id}/adapters/"
        metadata_key = f"{adapter_path}backup_metadata.json"
        
        try:
            metadata_obj = persistence_manager.s3_client.get_object(
                Bucket=persistence_manager.s3_bucket,
                Key=metadata_key
            )
            metadata = json.loads(metadata_obj['Body'].read().decode('utf-8'))
            
            # Also try to get adapter config if available
            try:
                with tempfile.TemporaryDirectory() as temp_dir:
                    local_adapter_path = os.path.join(temp_dir, "adapters")
                    await persistence_manager.restore_adapters_from_s3(local_adapter_path)
                    
                    config_path = os.path.join(local_adapter_path, "adapter_config.json")
                    if os.path.exists(config_path):
                        with open(config_path, 'r') as f:
                            adapter_config = json.load(f)
                        metadata["adapter_config"] = adapter_config
            except:
                pass
            
            return {
                "status": "found",
                "user_id": user_id,
                "avatar_id": avatar_id,
                "metadata": metadata
            }
            
        except Exception as e:
            if "404" in str(e) or "Not Found" in str(e):
                return {
                    "status": "not_found",
                    "user_id": user_id,
                    "avatar_id": avatar_id,
                    "message": "Adapter does not exist"
                }
            else:
                raise
        
    except Exception as e:
        logger.error(f"Error getting adapter info: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get adapter info: {str(e)}")

----

# api/persistence.py

"""
Adapter Persistence API router for S3 backup and restore operations
Modified to support dynamic user ID from environment variable
"""
from fastapi import APIRouter, HTTPException, Depends, status

from db.schema.models import (
    AdapterBackupResponse, 
    AdapterListBackupsResponse, 
    AdapterRestoreResponse,
)


from core.logging import logger

router = APIRouter()

# Routes - all using the dependency from main.py
@router.post("/adapters/backup/{avatar_id}",
    response_model=AdapterBackupResponse,
    status_code=status.HTTP_201_CREATED,
    summary="Backup adapters to S3",
    description="Create a backup of LoRA adapter files and upload to S3 (user_id from environment)"
)
async def backup_adapters_to_s3(
    avatar_id: str,
    local_adapter_path: str
):
    """Backup adapter files to S3"""
    try:
        from main import get_adapter_persistence_manager
        manager = get_adapter_persistence_manager(avatar_id)
        backup_info = await manager.backup_adapters_to_s3(local_adapter_path)
        
        return AdapterBackupResponse(
            success=True,
            message=f"Successfully backed up adapters for user {manager.user_id}, avatar {avatar_id}",
            backup_info=backup_info
        )
    except Exception as e:
        logger.error(f"Error backing up adapters: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to backup adapters: {str(e)}"
        )

@router.post("/adapters/training-data/backup/{avatar_id}",
    response_model=AdapterBackupResponse,
    status_code=status.HTTP_201_CREATED,
    summary="Backup training data to S3",
    description="Create a backup of adapter training data and upload to S3 (user_id from environment)"
)
async def backup_training_data_to_s3(
    avatar_id: str,
    local_training_data_path: str
):
    """Backup training data to S3"""
    try:
        from main import get_adapter_persistence_manager
        manager = get_adapter_persistence_manager(avatar_id)
        backup_info = await manager.backup_training_data_to_s3(local_training_data_path)
        
        return AdapterBackupResponse(
            success=True,
            message=f"Successfully backed up training data for user {manager.user_id}, avatar {avatar_id}",
            backup_info=backup_info
        )
    except Exception as e:
        logger.error(f"Error backing up training data: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to backup training data: {str(e)}"
        )

@router.post("/adapters/restore/{avatar_id}",
    response_model=AdapterRestoreResponse,
    summary="Restore adapters from S3",
    description="Restore LoRA adapter files from S3 backup (user_id from environment)"
)
async def restore_adapters_from_s3(
    avatar_id: str,
    local_adapter_path: str
):
    """Restore adapter files from S3"""
    try:
        from main import get_adapter_persistence_manager
        manager = get_adapter_persistence_manager(avatar_id)
        await manager.restore_adapters_from_s3(local_adapter_path)
        
        return AdapterRestoreResponse(
            success=True,
            message=f"Successfully restored adapters for user {manager.user_id}, avatar {avatar_id}"
        )
    except Exception as e:
        logger.error(f"Error restoring adapters: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to restore adapters: {str(e)}"
        )

@router.post("/adapters/training-data/restore/{avatar_id}",
    response_model=AdapterRestoreResponse,
    summary="Restore training data from S3",
    description="Restore adapter training data from S3 backup (user_id from environment)"
)
async def restore_training_data_from_s3(
    avatar_id: str,
    local_training_data_path: str
):
    """Restore training data from S3"""
    try:
        from main import get_adapter_persistence_manager
        manager = get_adapter_persistence_manager(avatar_id)
        await manager.restore_training_data_from_s3(local_training_data_path)
        
        return AdapterRestoreResponse(
            success=True,
            message=f"Successfully restored training data for user {manager.user_id}, avatar {avatar_id}"
        )
    except Exception as e:
        logger.error(f"Error restoring training data: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to restore training data: {str(e)}"
        )

@router.get("/adapters/backups/{avatar_id}",
    response_model=AdapterListBackupsResponse,
    summary="List available adapter backups",
    description="List all available adapter and training data backups for the avatar (user_id from environment)"
)
async def list_adapter_backups(
    avatar_id: str
):
    """List available adapter backups"""
    try:
        from main import get_adapter_persistence_manager
        manager = get_adapter_persistence_manager(avatar_id)
        backups = await manager.list_adapter_backups()
        
        return AdapterListBackupsResponse(
            backups=backups,
            count=len(backups)
        )
    except Exception as e:
        logger.error(f"Error listing adapter backups: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to list adapter backups: {str(e)}"
        )

@router.delete("/adapters/backup/{avatar_id}",
    summary="Delete adapter backup from S3",
    description="Delete adapter backup from S3 (user_id from environment)"
)
async def delete_adapter_backup(
    avatar_id: str,
    backup_type: str = "adapters"  # "adapters" or "training_data"
):
    """Delete adapter backup from S3"""
    try:
        from main import get_adapter_persistence_manager
        manager = get_adapter_persistence_manager(avatar_id)
        
        if backup_type == "adapters":
            backup_key = f"{manager._get_s3_adapter_path()}adapter_backup.zip"
            metadata_key = f"{manager._get_s3_adapter_path()}backup_metadata.json"
        elif backup_type == "training_data":
            backup_key = f"{manager._get_s3_training_data_path()}training_data_backup.zip"
            metadata_key = f"{manager._get_s3_training_data_path()}backup_metadata.json"
        else:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="backup_type must be 'adapters' or 'training_data'"
            )
        
        # Delete from S3
        manager.s3_client.delete_object(
            Bucket=manager.s3_bucket,
            Key=backup_key
        )
        manager.s3_client.delete_object(
            Bucket=manager.s3_bucket,
            Key=metadata_key
        )
        
        return {
            "success": True, 
            "message": f"Successfully deleted {backup_type} backup for user {manager.user_id}, avatar {avatar_id}"
        }
    except Exception as e:
        logger.error(f"Error deleting adapter backup: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to delete adapter backup: {str(e)}"
        )

@router.get("/adapters/status/{avatar_id}",
    summary="Get adapter persistence status",
    description="Get the status of S3 connectivity and adapter backup information (user_id from environment)"
)
async def get_adapter_persistence_status(
    avatar_id: str
):
    """Get adapter persistence status"""
    try:
        from main import get_adapter_persistence_manager
        manager = get_adapter_persistence_manager(avatar_id)
        
        # Test S3 connectivity
        s3_status = "connected"
        try:
            manager.s3_client.head_bucket(Bucket=manager.s3_bucket)
        except Exception:
            s3_status = "disconnected"
        
        # Check if backups exist
        adapter_backup_exists = False
        training_data_backup_exists = False
        
        try:
            manager.s3_client.head_object(
                Bucket=manager.s3_bucket,
                Key=f"{manager._get_s3_adapter_path()}adapter_backup.zip"
            )
            adapter_backup_exists = True
        except:
            pass
        
        try:
            manager.s3_client.head_object(
                Bucket=manager.s3_bucket,
                Key=f"{manager._get_s3_training_data_path()}training_data_backup.zip"
            )
            training_data_backup_exists = True
        except:
            pass
        
        return {
            "s3_status": s3_status,
            "s3_bucket": manager.s3_bucket,
            "adapter_backup_path": f"s3://{manager.s3_bucket}/{manager._get_s3_adapter_path()}",
            "training_data_backup_path": f"s3://{manager.s3_bucket}/{manager._get_s3_training_data_path()}",
            "adapter_backup_exists": adapter_backup_exists,
            "training_data_backup_exists": training_data_backup_exists,
            "user_id": manager.user_id,
            "avatar_id": avatar_id
        }
    except Exception as e:
        logger.error(f"Error getting adapter persistence status: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get adapter persistence status: {str(e)}"
        )
---

from fastapi import APIRouter, UploadFile, File, HTTPException, Query
from fastapi.responses import JSONResponse
from typing import List, Optional
import json
import os
from datetime import datetime

from service.s3_service import S3Service
from service.persistence_service import get_adapter_persistence_manager
from db.schema.models import S3UploadRequest, MetadataUpdate, TrainingDataMetadata
from core.logging import logger

router = APIRouter()
s3_service = S3Service()

@router.post("/{user_id}/{avatar_id}/upload")
async def upload_training_data(
    user_id: str,
    avatar_id: str,
    file: UploadFile = File(...),
    use_for_training: bool = Query(True)
):
    """Upload training data file"""
    try:
        # Get persistence manager
        persistence_manager = get_adapter_persistence_manager(avatar_id)
        
        # Define S3 paths
        training_data_path = f"users/{user_id}/avatars/{avatar_id}/adapters/training_data/"
        metadata_path = f"users/{user_id}/avatars/{avatar_id}/adapters/metadata/"
        
        # Upload file to S3
        file_key = f"{training_data_path}{file.filename}"
        
        # Read file content
        file_content = await file.read()
        
        # Upload to S3
        persistence_manager.s3_client.put_object(
            Bucket=persistence_manager.s3_bucket,
            Key=file_key,
            Body=file_content,
            ContentType=file.content_type or 'application/octet-stream',
            Metadata={
                'user_id': user_id,
                'avatar_id': avatar_id,
                'upload_timestamp': datetime.now().isoformat(),
                'original_filename': file.filename,
                'use_for_training': str(use_for_training)
            }
        )
        
        # Update metadata.json
        metadata_key = f"{metadata_path}metadata.json"
        
        # Get existing metadata or create new
        try:
            metadata_obj = persistence_manager.s3_client.get_object(
                Bucket=persistence_manager.s3_bucket,
                Key=metadata_key
            )
            metadata = json.loads(metadata_obj['Body'].read().decode('utf-8'))
        except:
            metadata = {}
        
        # Update metadata for this file
        metadata[file.filename] = use_for_training
        
        # Upload updated metadata
        persistence_manager.s3_client.put_object(
            Bucket=persistence_manager.s3_bucket,
            Key=metadata_key,
            Body=json.dumps(metadata, indent=2),
            ContentType='application/json'
        )
        
        logger.info(f"Uploaded training file {file.filename} for user {user_id}, avatar {avatar_id}")
        
        return {
            "status": "success",
            "message": f"File {file.filename} uploaded successfully",
            "file_size": len(file_content),
            "use_for_training": use_for_training,
            "s3_key": file_key
        }
        
    except Exception as e:
        logger.error(f"Error uploading training data: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to upload training data: {str(e)}")

@router.get("/{user_id}/{avatar_id}/list")
async def list_training_data(
    user_id: str,
    avatar_id: str,
    training_only: Optional[bool] = Query(None)
) -> List[TrainingDataMetadata]:
    """List training data files with optional filtering"""
    try:
        # Get persistence manager
        persistence_manager = get_adapter_persistence_manager(avatar_id)
        
        # Define S3 paths
        training_data_path = f"users/{user_id}/avatars/{avatar_id}/adapters/training_data/"
        metadata_path = f"users/{user_id}/avatars/{avatar_id}/adapters/metadata/"
        
        # Get metadata
        metadata_key = f"{metadata_path}metadata.json"
        training_metadata = {}
        
        try:
            metadata_obj = persistence_manager.s3_client.get_object(
                Bucket=persistence_manager.s3_bucket,
                Key=metadata_key
            )
            training_metadata = json.loads(metadata_obj['Body'].read().decode('utf-8'))
        except Exception as e:
            logger.warning(f"No training metadata found: {e}")
        
        # List files in training data directory
        response = persistence_manager.s3_client.list_objects_v2(
            Bucket=persistence_manager.s3_bucket,
            Prefix=training_data_path
        )
        
        files_list = []
        
        if 'Contents' in response:
            for obj in response['Contents']:
                # Skip directory-like objects
                if obj['Key'].endswith('/'):
                    continue
                
                filename = os.path.basename(obj['Key'])
                use_for_training = training_metadata.get(filename, False)
                
                # Apply training_only filter
                if training_only is not None:
                    if training_only and not use_for_training:
                        continue
                    elif not training_only and use_for_training:
                        continue
                
                # Get file metadata from S3 object metadata
                try:
                    head_response = persistence_manager.s3_client.head_object(
                        Bucket=persistence_manager.s3_bucket,
                        Key=obj['Key']
                    )
                    file_metadata = head_response.get('Metadata', {})
                except:
                    file_metadata = {}
                
                files_list.append(TrainingDataMetadata(
                    filename=filename,
                    use_for_training=use_for_training,
                    file_size=obj['Size'],
                    last_modified=obj['LastModified'],
                    content_type=file_metadata.get('content-type', 'unknown'),
                    upload_timestamp=file_metadata.get('upload_timestamp'),
                    s3_key=obj['Key']
                ))
        
        logger.info(f"Listed {len(files_list)} training data files for user {user_id}, avatar {avatar_id}")
        
        return files_list
        
    except Exception as e:
        logger.error(f"Error listing training data: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to list training data: {str(e)}")

@router.put("/{user_id}/{avatar_id}/{file_name}/training-flag")
async def update_training_flag(
    user_id: str,
    avatar_id: str,
    file_name: str,
    update: MetadataUpdate
):
    """Update whether a file should be used for training"""
    try:
        # Get persistence manager
        persistence_manager = get_adapter_persistence_manager(avatar_id)
        
        # Define S3 paths
        training_data_path = f"users/{user_id}/avatars/{avatar_id}/adapters/training_data/"
        metadata_path = f"users/{user_id}/avatars/{avatar_id}/adapters/metadata/"
        
        # Check if file exists
        file_key = f"{training_data_path}{file_name}"
        try:
            persistence_manager.s3_client.head_object(
                Bucket=persistence_manager.s3_bucket,
                Key=file_key
            )
        except:
            raise HTTPException(
                status_code=404,
                detail=f"Training data file {file_name} not found"
            )
        
        # Get existing metadata
        metadata_key = f"{metadata_path}metadata.json"
        
        try:
            metadata_obj = persistence_manager.s3_client.get_object(
                Bucket=persistence_manager.s3_bucket,
                Key=metadata_key
            )
            metadata = json.loads(metadata_obj['Body'].read().decode('utf-8'))
        except:
            metadata = {}
        
        # Update metadata for this file
        old_value = metadata.get(file_name, False)
        metadata[file_name] = update.use_for_training
        
        # Upload updated metadata
        persistence_manager.s3_client.put_object(
            Bucket=persistence_manager.s3_bucket,
            Key=metadata_key,
            Body=json.dumps(metadata, indent=2),
            ContentType='application/json'
        )
        
        logger.info(f"Updated training flag for {file_name}: {old_value} -> {update.use_for_training}")
        
        return {
            "status": "success",
            "message": f"Training flag updated for {file_name}",
            "filename": file_name,
            "old_value": old_value,
            "new_value": update.use_for_training
        }
        
    except Exception as e:
        logger.error(f"Error updating training flag: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to update training flag: {str(e)}")

@router.delete("/{user_id}/{avatar_id}/{file_name}")
async def delete_training_file(user_id: str, avatar_id: str, file_name: str):
    """Delete a specific training file"""
    try:
        # Get persistence manager
        persistence_manager = get_adapter_persistence_manager(avatar_id)
        
        # Define S3 paths
        training_data_path = f"users/{user_id}/avatars/{avatar_id}/adapters/training_data/"
        metadata_path = f"users/{user_id}/avatars/{avatar_id}/adapters/metadata/"
        
        # Check if file exists
        file_key = f"{training_data_path}{file_name}"
        try:
            persistence_manager.s3_client.head_object(
                Bucket=persistence_manager.s3_bucket,
                Key=file_key
            )
        except:
            raise HTTPException(
                status_code=404,
                detail=f"Training data file {file_name} not found"
            )
        
        # Delete the file
        persistence_manager.s3_client.delete_object(
            Bucket=persistence_manager.s3_bucket,
            Key=file_key
        )
        
        # Update metadata by removing the file entry
        metadata_key = f"{metadata_path}metadata.json"
        
        try:
            metadata_obj = persistence_manager.s3_client.get_object(
                Bucket=persistence_manager.s3_bucket,
                Key=metadata_key
            )
            metadata = json.loads(metadata_obj['Body'].read().decode('utf-8'))
            
            # Remove file from metadata
            if file_name in metadata:
                del metadata[file_name]
                
                # Upload updated metadata
                persistence_manager.s3_client.put_object(
                    Bucket=persistence_manager.s3_bucket,
                    Key=metadata_key,
                    Body=json.dumps(metadata, indent=2),
                    ContentType='application/json'
                )
        except Exception as e:
            logger.warning(f"Could not update metadata after file deletion: {e}")
        
        logger.info(f"Deleted training file {file_name} for user {user_id}, avatar {avatar_id}")
        
        return {
            "status": "success",
            "message": f"Training file {file_name} deleted successfully",
            "filename": file_name
        }
        
    except Exception as e:
        logger.error(f"Error deleting training file: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to delete training file: {str(e)}")

@router.get("/{user_id}/{avatar_id}/download/{file_name}")
async def download_training_file(user_id: str, avatar_id: str, file_name: str):
    """Download a specific training data file"""
    try:
        # Get persistence manager
        persistence_manager = get_adapter_persistence_manager(avatar_id)
        
        # Define S3 path
        training_data_path = f"users/{user_id}/avatars/{avatar_id}/adapters/training_data/"
        file_key = f"{training_data_path}{file_name}"
        
        # Check if file exists
        try:
            head_response = persistence_manager.s3_client.head_object(
                Bucket=persistence_manager.s3_bucket,
                Key=file_key
            )
        except:
            raise HTTPException(
                status_code=404,
                detail=f"Training data file {file_name} not found"
            )
        
        # Generate presigned URL for download
        try:
            download_url = persistence_manager.s3_client.generate_presigned_url(
                'get_object',
                Params={'Bucket': persistence_manager.s3_bucket, 'Key': file_key},
                ExpiresIn=3600  # 1 hour
            )
            
            return {
                "status": "success",
                "filename": file_name,
                "download_url": download_url,
                "expires_in": 3600,
                "file_size": head_response['ContentLength'],
                "last_modified": head_response['LastModified'].isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error generating presigned URL: {e}")
            raise HTTPException(
                status_code=500,
                detail="Failed to generate download URL"
            )
        
    except Exception as e:
        logger.error(f"Error downloading training file: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to download training file: {str(e)}")

@router.get("/{user_id}/{avatar_id}/metadata")
async def get_training_metadata(user_id: str, avatar_id: str):
    """Get the complete training metadata for an avatar"""
    try:
        # Get persistence manager
        persistence_manager = get_adapter_persistence_manager(avatar_id)
        
        # Define S3 path
        metadata_path = f"users/{user_id}/avatars/{avatar_id}/adapters/metadata/"
        metadata_key = f"{metadata_path}metadata.json"
        
        try:
            metadata_obj = persistence_manager.s3_client.get_object(
                Bucket=persistence_manager.s3_bucket,
                Key=metadata_key
            )
            metadata = json.loads(metadata_obj['Body'].read().decode('utf-8'))
            
            # Count files by training status
            training_files = sum(1 for use_training in metadata.values() if use_training)
            non_training_files = len(metadata) - training_files
            
            return {
                "status": "found",
                "user_id": user_id,
                "avatar_id": avatar_id,
                "metadata": metadata,
                "summary": {
                    "total_files": len(metadata),
                    "training_files": training_files,
                    "non_training_files": non_training_files
                }
            }
            
        except Exception as e:
            if "404" in str(e) or "Not Found" in str(e):
                return {
                    "status": "not_found",
                    "user_id": user_id,
                    "avatar_id": avatar_id,
                    "message": "No training metadata found",
                    "metadata": {},
                    "summary": {
                        "total_files": 0,
                        "training_files": 0,
                        "non_training_files": 0
                    }
                }
            else:
                raise
        
    except Exception as e:
        logger.error(f"Error getting training metadata: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get training metadata: {str(e)}")

@router.delete("/{user_id}/{avatar_id}/non-training-files")
async def delete_non_training_files(user_id: str, avatar_id: str):
    """Delete all files not marked for training"""
    try:
        # Get persistence manager
        persistence_manager = get_adapter_persistence_manager(avatar_id)
        
        # Define S3 paths
        training_data_path = f"users/{user_id}/avatars/{avatar_id}/adapters/training_data/"
        metadata_path = f"users/{user_id}/avatars/{avatar_id}/adapters/metadata/"
        
        # Get metadata
        metadata_key = f"{metadata_path}metadata.json"
        training_metadata = {}
        
        try:
            metadata_obj = persistence_manager.s3_client.get_object(
                Bucket=persistence_manager.s3_bucket,
                Key=metadata_key
            )
            training_metadata = json.loads(metadata_obj['Body'].read().decode('utf-8'))
        except Exception as e:
            logger.warning(f"No training metadata found: {e}")
            return {
                "status": "success",
                "message": "No metadata found, no files to delete",
                "deleted_files": []
            }
        
        # List files in training data directory
        response = persistence_manager.s3_client.list_objects_v2(
            Bucket=persistence_manager.s3_bucket,
            Prefix=training_data_path
        )
        
        files_to_delete = []
        deleted_files = []
        
        if 'Contents' in response:
            for obj in response['Contents']:
                # Skip directory-like objects
                if obj['Key'].endswith('/'):
                    continue
                
                filename = os.path.basename(obj['Key'])
                use_for_training = training_metadata.get(filename, False)
                
                # If not marked for training, mark for deletion
                if not use_for_training:
                    files_to_delete.append({'Key': obj['Key']})
                    deleted_files.append(filename)
        
        # Delete files
        if files_to_delete:
            persistence_manager.s3_client.delete_objects(
                Bucket=persistence_manager.s3_bucket,
                Delete={'Objects': files_to_delete}
            )
            
            # Update metadata by removing deleted files
            updated_metadata = {
                filename: use_for_training 
                for filename, use_for_training in training_metadata.items()
                if filename not in deleted_files
            }
            
            # Upload updated metadata
            persistence_manager.s3_client.put_object(
                Bucket=persistence_manager.s3_bucket,
                Key=metadata_key,
                Body=json.dumps(updated_metadata, indent=2),
                ContentType='application/json'
            )
        
        logger.info(f"Deleted {len(deleted_files)} non-training files for user {user_id}, avatar {avatar_id}")
        
        return {
            "status": "success",
            "message": f"Deleted {len(deleted_files)} files not marked for training",
            "deleted_files": deleted_files
        }
        
    except Exception as e:
        logger.error(f"Error deleting non-training files: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to delete non-training files: {str(e)}")
