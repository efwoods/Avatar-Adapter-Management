# duplication of persistence; need to reduce complexity and allow for the useful persitence, crud of adapters, crud of training, and training of adapters

I need to only use the AdapterPersistenceManager for C.R.U.D. persistence operations of both adapters and adapter training documents. 


---

# api/adapters.py

from fastapi import APIRouter, HTTPException, Depends
from fastapi.responses import FileResponse
from typing import Optional, Dict, Any
import os
import tempfile
import json
from datetime import datetime

from classes.lora_manager import LoRAManager
from db.schema.models import TrainingRequest, AdapterConfig
from service.persistence_service import get_adapter_persistence_manager
from service.training_service import TrainingService
from service.s3_service import S3Service
from core.logging import logger

router = APIRouter()
lora_manager = LoRAManager()
training_service = TrainingService()
s3_service = S3Service()

@router.post("/{user_id}/{avatar_id}/create")
async def create_adapter(
    user_id: str, 
    avatar_id: str, 
    adapter_name: str = "default"
) -> AdapterConfig:
    """Create a new adapter configuration"""
    try:
        # Get persistence manager
        persistence_manager = get_adapter_persistence_manager(avatar_id)
        
        # Check if adapter already exists
        adapter_path = f"users/{user_id}/avatars/{avatar_id}/adapters/"
        
        try:
            # Try to list existing adapters
            response = persistence_manager.s3_client.list_objects_v2(
                Bucket=persistence_manager.s3_bucket,
                Prefix=adapter_path
            )
            
            if 'Contents' in response and any(obj['Key'].endswith('adapter_backup.zip') for obj in response['Contents']):
                logger.info(f"Adapter already exists for user {user_id}, avatar {avatar_id}")
                # Return existing adapter config
                return AdapterConfig(
                    user_id=user_id,
                    avatar_id=avatar_id,
                    adapter_name=adapter_name,
                    status="existing",
                    created_at=datetime.now(),
                    s3_path=adapter_path
                )
        except Exception as e:
            logger.warning(f"Error checking existing adapter: {e}")
        
        # Create new adapter locally
        with tempfile.TemporaryDirectory() as temp_dir:
            local_adapter_path = os.path.join(temp_dir, "adapters")
            os.makedirs(local_adapter_path, exist_ok=True)
            
            # Initialize empty adapter structure
            adapter_config = {
                "adapter_name": adapter_name,
                "user_id": user_id,
                "avatar_id": avatar_id,
                "created_at": datetime.now().isoformat(),
                "version": "1.0.0",
                "status": "untrained",
                "training_history": []
            }
            
            # Save adapter config
            config_path = os.path.join(local_adapter_path, "adapter_config.json")
            with open(config_path, 'w') as f:
                json.dump(adapter_config, f, indent=2)
            
            # Create placeholder adapter files (LoRA specific structure)
            lora_structure = {
                "adapter_model.bin": b"",  # Placeholder for actual adapter weights
                "adapter_config.json": json.dumps({
                    "target_modules": ["q_proj", "v_proj"],
                    "r": 16,
                    "lora_alpha": 32,
                    "lora_dropout": 0.1
                }).encode()
            }
            
            for filename, content in lora_structure.items():
                file_path = os.path.join(local_adapter_path, filename)
                with open(file_path, 'wb') as f:
                    f.write(content)
            
            # Backup to S3
            backup_metadata = await persistence_manager.backup_adapters_to_s3(local_adapter_path)
            
            logger.info(f"Created and backed up new adapter for user {user_id}, avatar {avatar_id}")
            
            return AdapterConfig(
                user_id=user_id,
                avatar_id=avatar_id,
                adapter_name=adapter_name,
                status="created",
                created_at=datetime.now(),
                s3_path=adapter_path,
                metadata=backup_metadata
            )
            
    except Exception as e:
        logger.error(f"Error creating adapter: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to create adapter: {str(e)}")

@router.post("/{user_id}/{avatar_id}/train")
async def train_adapter(
    user_id: str, 
    avatar_id: str, 
    training_params: Optional[Dict] = None
):
    """Train a LoRA adapter"""
    try:
        # Get persistence manager
        persistence_manager = get_adapter_persistence_manager(avatar_id)
        
        # Get metadata to determine which files to use for training
        metadata_path = f"users/{user_id}/avatars/{avatar_id}/adapters/metadata/"
        metadata_key = f"{metadata_path}metadata.json"
        
        training_files = []
        try:
            # Get training metadata
            metadata_obj = persistence_manager.s3_client.get_object(
                Bucket=persistence_manager.s3_bucket,
                Key=metadata_key
            )
            metadata = json.loads(metadata_obj['Body'].read().decode('utf-8'))
            
            # Filter files marked for training
            training_files = [filename for filename, use_for_training in metadata.items() if use_for_training]
            logger.info(f"Found {len(training_files)} files marked for training")
            
        except Exception as e:
            logger.warning(f"No training metadata found: {e}. Proceeding without training data.")
        
        with tempfile.TemporaryDirectory() as temp_dir:
            local_adapter_path = os.path.join(temp_dir, "adapters")
            local_training_path = os.path.join(temp_dir, "training_data")
            os.makedirs(local_adapter_path, exist_ok=True)
            os.makedirs(local_training_path, exist_ok=True)
            
            # Restore existing adapter or create new one
            try:
                await persistence_manager.restore_adapters_from_s3(local_adapter_path)
                logger.info("Restored existing adapter from S3")
            except HTTPException as e:
                if e.status_code == 404:
                    # Create new adapter if none exists
                    logger.info("No existing adapter found, creating new one")
                    await create_adapter(user_id, avatar_id)
                    await persistence_manager.restore_adapters_from_s3(local_adapter_path)
                else:
                    raise
            
            # Download training data if available
            if training_files:
                training_data_path = f"users/{user_id}/avatars/{avatar_id}/adapters/training_data/"
                
                for filename in training_files:
                    try:
                        file_key = f"{training_data_path}{filename}"
                        local_file_path = os.path.join(local_training_path, filename)
                        
                        persistence_manager.s3_client.download_file(
                            persistence_manager.s3_bucket,
                            file_key,
                            local_file_path
                        )
                        logger.info(f"Downloaded training file: {filename}")
                    except Exception as e:
                        logger.warning(f"Failed to download training file {filename}: {e}")
            
            # Train the adapter
            if training_files and os.listdir(local_training_path):
                try:
                    # Use training service to train the adapter
                    training_result = await training_service.train_lora_adapter(
                        adapter_path=local_adapter_path,
                        training_data_path=local_training_path,
                        training_params=training_params or {}
                    )
                    
                    # Update adapter config with training info
                    config_path = os.path.join(local_adapter_path, "adapter_config.json")
                    if os.path.exists(config_path):
                        with open(config_path, 'r') as f:
                            adapter_config = json.load(f)
                        
                        adapter_config.update({
                            "status": "trained",
                            "last_training": datetime.now().isoformat(),
                            "training_files_used": training_files,
                            "training_result": training_result
                        })
                        
                        # Add to training history
                        if "training_history" not in adapter_config:
                            adapter_config["training_history"] = []
                        
                        adapter_config["training_history"].append({
                            "timestamp": datetime.now().isoformat(),
                            "files_used": training_files,
                            "training_params": training_params,
                            "result": training_result
                        })
                        
                        with open(config_path, 'w') as f:
                            json.dump(adapter_config, f, indent=2)
                    
                    logger.info(f"Successfully trained adapter for user {user_id}, avatar {avatar_id}")
                    
                except Exception as e:
                    logger.error(f"Training failed: {e}")
                    # Update status to indicate training failure
                    config_path = os.path.join(local_adapter_path, "adapter_config.json")
                    if os.path.exists(config_path):
                        with open(config_path, 'r') as f:
                            adapter_config = json.load(f)
                        adapter_config["status"] = "training_failed"
                        adapter_config["last_error"] = str(e)
                        with open(config_path, 'w') as f:
                            json.dump(adapter_config, f, indent=2)
            else:
                logger.info("No training data available, uploading untrained adapter")
                # Update status to untrained
                config_path = os.path.join(local_adapter_path, "adapter_config.json")
                if os.path.exists(config_path):
                    with open(config_path, 'r') as f:
                        adapter_config = json.load(f)
                    adapter_config["status"] = "untrained"
                    adapter_config["last_updated"] = datetime.now().isoformat()
                    with open(config_path, 'w') as f:
                        json.dump(adapter_config, f, indent=2)
            
            # Backup trained/updated adapter to S3
            backup_metadata = await persistence_manager.backup_adapters_to_s3(local_adapter_path)
            
            return {
                "status": "success",
                "message": f"Adapter training completed for user {user_id}, avatar {avatar_id}",
                "training_files_used": training_files,
                "backup_metadata": backup_metadata
            }
            
    except Exception as e:
        logger.error(f"Error training adapter: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to train adapter: {str(e)}")

@router.delete("/{user_id}/{avatar_id}")
async def delete_adapter(user_id: str, avatar_id: str):
    """Delete an adapter"""
    try:
        # Get persistence manager
        persistence_manager = get_adapter_persistence_manager(avatar_id)
        
        # Define S3 paths to delete
        adapter_path = f"users/{user_id}/avatars/{avatar_id}/adapters/"
        
        # List all objects with the adapter prefix
        response = persistence_manager.s3_client.list_objects_v2(
            Bucket=persistence_manager.s3_bucket,
            Prefix=adapter_path
        )
        
        if 'Contents' not in response:
            raise HTTPException(
                status_code=404,
                detail=f"No adapter found for user {user_id}, avatar {avatar_id}"
            )
        
        # Delete all adapter-related objects
        objects_to_delete = [{'Key': obj['Key']} for obj in response['Contents']]
        
        if objects_to_delete:
            persistence_manager.s3_client.delete_objects(
                Bucket=persistence_manager.s3_bucket,
                Delete={'Objects': objects_to_delete}
            )
            
            logger.info(f"Deleted {len(objects_to_delete)} adapter objects for user {user_id}, avatar {avatar_id}")
        
        return {
            "status": "success",
            "message": f"Adapter deleted for user {user_id}, avatar {avatar_id}",
            "deleted_objects": len(objects_to_delete)
        }
        
    except Exception as e:
        logger.error(f"Error deleting adapter: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to delete adapter: {str(e)}")

@router.get("/{user_id}/{avatar_id}")
async def get_adapter(user_id: str, avatar_id: str):
    """Get an adapter - returns adapter file for download or creates new one if doesn't exist"""
    try:
        # Get persistence manager
        persistence_manager = get_adapter_persistence_manager(avatar_id)
        
        # Check if adapter exists
        adapter_path = f"users/{user_id}/avatars/{avatar_id}/adapters/"
        adapter_key = f"{adapter_path}adapter_backup.zip"
        
        try:
            # Check if adapter exists in S3
            persistence_manager.s3_client.head_object(
                Bucket=persistence_manager.s3_bucket,
                Key=adapter_key
            )
            
            # Adapter exists, download and return
            with tempfile.NamedTemporaryFile(delete=False, suffix='.zip') as temp_file:
                persistence_manager.s3_client.download_file(
                    persistence_manager.s3_bucket,
                    adapter_key,
                    temp_file.name
                )
                
                # Get adapter metadata
                try:
                    metadata_key = f"{adapter_path}backup_metadata.json"
                    metadata_obj = persistence_manager.s3_client.get_object(
                        Bucket=persistence_manager.s3_bucket,
                        Key=metadata_key
                    )
                    metadata = json.loads(metadata_obj['Body'].read().decode('utf-8'))
                except:
                    metadata = {}
                
                logger.info(f"Retrieved existing adapter for user {user_id}, avatar {avatar_id}")
                
                # Return file response for download
                return FileResponse(
                    path=temp_file.name,
                    filename=f"adapter_{user_id}_{avatar_id}.zip",
                    media_type="application/zip",
                    headers={
                        "X-Adapter-Metadata": json.dumps(metadata),
                        "X-User-ID": user_id,
                        "X-Avatar-ID": avatar_id
                    }
                )
                
        except Exception as e:
            if "404" in str(e) or "Not Found" in str(e):
                # Adapter doesn't exist, create new one
                logger.info(f"No existing adapter found for user {user_id}, avatar {avatar_id}, creating new one")
                
                # Create new adapter
                adapter_config = await create_adapter(user_id, avatar_id)
                
                # Now retrieve the newly created adapter
                with tempfile.NamedTemporaryFile(delete=False, suffix='.zip') as temp_file:
                    persistence_manager.s3_client.download_file(
                        persistence_manager.s3_bucket,
                        adapter_key,
                        temp_file.name
                    )
                    
                    return FileResponse(
                        path=temp_file.name,
                        filename=f"adapter_{user_id}_{avatar_id}.zip",
                        media_type="application/zip",
                        headers={
                            "X-Adapter-Status": "newly_created",
                            "X-User-ID": user_id,
                            "X-Avatar-ID": avatar_id
                        }
                    )
            else:
                raise
        
    except Exception as e:
        logger.error(f"Error getting adapter: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get adapter: {str(e)}")

@router.get("/{user_id}/{avatar_id}/info")
async def get_adapter_info(user_id: str, avatar_id: str):
    """Get adapter information without downloading the file"""
    try:
        # Get persistence manager
        persistence_manager = get_adapter_persistence_manager(avatar_id)
        
        # Get adapter metadata
        adapter_path = f"users/{user_id}/avatars/{avatar_id}/adapters/"
        metadata_key = f"{adapter_path}backup_metadata.json"
        
        try:
            metadata_obj = persistence_manager.s3_client.get_object(
                Bucket=persistence_manager.s3_bucket,
                Key=metadata_key
            )
            metadata = json.loads(metadata_obj['Body'].read().decode('utf-8'))
            
            # Also try to get adapter config if available
            try:
                with tempfile.TemporaryDirectory() as temp_dir:
                    local_adapter_path = os.path.join(temp_dir, "adapters")
                    await persistence_manager.restore_adapters_from_s3(local_adapter_path)
                    
                    config_path = os.path.join(local_adapter_path, "adapter_config.json")
                    if os.path.exists(config_path):
                        with open(config_path, 'r') as f:
                            adapter_config = json.load(f)
                        metadata["adapter_config"] = adapter_config
            except:
                pass
            
            return {
                "status": "found",
                "user_id": user_id,
                "avatar_id": avatar_id,
                "metadata": metadata
            }
            
        except Exception as e:
            if "404" in str(e) or "Not Found" in str(e):
                return {
                    "status": "not_found",
                    "user_id": user_id,
                    "avatar_id": avatar_id,
                    "message": "Adapter does not exist"
                }
            else:
                raise
        
    except Exception as e:
        logger.error(f"Error getting adapter info: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get adapter info: {str(e)}")

___


# api/persistence.py

"""
Adapter Persistence API router for S3 backup and restore operations
Modified to support dynamic user ID from environment variable
"""
from fastapi import APIRouter, HTTPException, Depends, status

from db.schema.models import (
    AdapterBackupResponse, 
    AdapterListBackupsResponse, 
    AdapterRestoreResponse,
)

from service.persistence_service import get_adapter_persistence_manager
from core.logging import logger

router = APIRouter()

# Routes - all using the dependency from service.py
@router.post("/adapters/backup/{avatar_id}",
    response_model=AdapterBackupResponse,
    status_code=status.HTTP_201_CREATED,
    summary="Backup adapters to S3",
    description="Create a backup of LoRA adapter files and upload to S3 (user_id from environment)"
)
async def backup_adapters_to_s3(
    avatar_id: str,
    local_adapter_path: str
):
    """Backup adapter files to S3"""
    try:
        
        manager = get_adapter_persistence_manager(avatar_id)
        backup_info = await manager.backup_adapters_to_s3(local_adapter_path)
        
        return AdapterBackupResponse(
            success=True,
            message=f"Successfully backed up adapters for user {manager.user_id}, avatar {avatar_id}",
            backup_info=backup_info
        )
    except Exception as e:
        logger.error(f"Error backing up adapters: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to backup adapters: {str(e)}"
        )

@router.post("/adapters/training-data/backup/{avatar_id}",
    response_model=AdapterBackupResponse,
    status_code=status.HTTP_201_CREATED,
    summary="Backup training data to S3",
    description="Create a backup of adapter training data and upload to S3 (user_id from environment)"
)
async def backup_training_data_to_s3(
    avatar_id: str,
    local_training_data_path: str
):
    """Backup training data to S3"""
    try:
        
        manager = get_adapter_persistence_manager(avatar_id)
        backup_info = await manager.backup_training_data_to_s3(local_training_data_path)
        
        return AdapterBackupResponse(
            success=True,
            message=f"Successfully backed up training data for user {manager.user_id}, avatar {avatar_id}",
            backup_info=backup_info
        )
    except Exception as e:
        logger.error(f"Error backing up training data: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to backup training data: {str(e)}"
        )

@router.post("/adapters/restore/{avatar_id}",
    response_model=AdapterRestoreResponse,
    summary="Restore adapters from S3",
    description="Restore LoRA adapter files from S3 backup (user_id from environment)"
)
async def restore_adapters_from_s3(
    avatar_id: str,
    local_adapter_path: str
):
    """Restore adapter files from S3"""
    try:
        
        manager = get_adapter_persistence_manager(avatar_id)
        await manager.restore_adapters_from_s3(local_adapter_path)
        
        return AdapterRestoreResponse(
            success=True,
            message=f"Successfully restored adapters for user {manager.user_id}, avatar {avatar_id}"
        )
    except Exception as e:
        logger.error(f"Error restoring adapters: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to restore adapters: {str(e)}"
        )

@router.post("/adapters/training-data/restore/{avatar_id}",
    response_model=AdapterRestoreResponse,
    summary="Restore training data from S3",
    description="Restore adapter training data from S3 backup (user_id from environment)"
)
async def restore_training_data_from_s3(
    avatar_id: str,
    local_training_data_path: str
):
    """Restore training data from S3"""
    try:
        
        manager = get_adapter_persistence_manager(avatar_id)
        await manager.restore_training_data_from_s3(local_training_data_path)
        
        return AdapterRestoreResponse(
            success=True,
            message=f"Successfully restored training data for user {manager.user_id}, avatar {avatar_id}"
        )
    except Exception as e:
        logger.error(f"Error restoring training data: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to restore training data: {str(e)}"
        )

@router.get("/adapters/backups/{avatar_id}",
    response_model=AdapterListBackupsResponse,
    summary="List available adapter backups",
    description="List all available adapter and training data backups for the avatar (user_id from environment)"
)
async def list_adapter_backups(
    avatar_id: str
):
    """List available adapter backups"""
    try:
        
        manager = get_adapter_persistence_manager(avatar_id)
        backups = await manager.list_adapter_backups()
        
        return AdapterListBackupsResponse(
            backups=backups,
            count=len(backups)
        )
    except Exception as e:
        logger.error(f"Error listing adapter backups: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to list adapter backups: {str(e)}"
        )

@router.delete("/adapters/backup/{avatar_id}",
    summary="Delete adapter backup from S3",
    description="Delete adapter backup from S3 (user_id from environment)"
)
async def delete_adapter_backup(
    avatar_id: str,
    backup_type: str = "adapters"  # "adapters" or "training_data"
):
    """Delete adapter backup from S3"""
    try:
        
        manager = get_adapter_persistence_manager(avatar_id)
        
        if backup_type == "adapters":
            backup_key = f"{manager._get_s3_adapter_path()}adapter_backup.zip"
            metadata_key = f"{manager._get_s3_adapter_path()}backup_metadata.json"
        elif backup_type == "training_data":
            backup_key = f"{manager._get_s3_training_data_path()}training_data_backup.zip"
            metadata_key = f"{manager._get_s3_training_data_path()}backup_metadata.json"
        else:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="backup_type must be 'adapters' or 'training_data'"
            )
        
        # Delete from S3
        manager.s3_client.delete_object(
            Bucket=manager.s3_bucket,
            Key=backup_key
        )
        manager.s3_client.delete_object(
            Bucket=manager.s3_bucket,
            Key=metadata_key
        )
        
        return {
            "success": True, 
            "message": f"Successfully deleted {backup_type} backup for user {manager.user_id}, avatar {avatar_id}"
        }
    except Exception as e:
        logger.error(f"Error deleting adapter backup: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to delete adapter backup: {str(e)}"
        )

@router.get("/adapters/status/{avatar_id}",
    summary="Get adapter persistence status",
    description="Get the status of S3 connectivity and adapter backup information (user_id from environment)"
)
async def get_adapter_persistence_status(
    avatar_id: str
):
    """Get adapter persistence status"""
    try:
        
        manager = get_adapter_persistence_manager(avatar_id)
        
        # Test S3 connectivity
        s3_status = "connected"
        try:
            manager.s3_client.head_bucket(Bucket=manager.s3_bucket)
        except Exception:
            s3_status = "disconnected"
        
        # Check if backups exist
        adapter_backup_exists = False
        training_data_backup_exists = False
        
        try:
            manager.s3_client.head_object(
                Bucket=manager.s3_bucket,
                Key=f"{manager._get_s3_adapter_path()}adapter_backup.zip"
            )
            adapter_backup_exists = True
        except:
            pass
        
        try:
            manager.s3_client.head_object(
                Bucket=manager.s3_bucket,
                Key=f"{manager._get_s3_training_data_path()}training_data_backup.zip"
            )
            training_data_backup_exists = True
        except:
            pass
        
        return {
            "s3_status": s3_status,
            "s3_bucket": manager.s3_bucket,
            "adapter_backup_path": f"s3://{manager.s3_bucket}/{manager._get_s3_adapter_path()}",
            "training_data_backup_path": f"s3://{manager.s3_bucket}/{manager._get_s3_training_data_path()}",
            "adapter_backup_exists": adapter_backup_exists,
            "training_data_backup_exists": training_data_backup_exists,
            "user_id": manager.user_id,
            "avatar_id": avatar_id
        }
    except Exception as e:
        logger.error(f"Error getting adapter persistence status: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get adapter persistence status: {str(e)}"
        )


----


# api/training_data.py

from fastapi import APIRouter, UploadFile, File, HTTPException, Query
from fastapi.responses import JSONResponse
from typing import List, Optional
import json
import os
from datetime import datetime

from service.s3_service import S3Service
from service.persistence_service import get_adapter_persistence_manager
from db.schema.models import S3UploadRequest, MetadataUpdate, TrainingDataMetadata
from core.logging import logger

router = APIRouter()
s3_service = S3Service()

@router.post("/{user_id}/{avatar_id}/upload")
async def upload_training_data(
    user_id: str,
    avatar_id: str,
    file: UploadFile = File(...),
    use_for_training: bool = Query(True)
):
    """Upload training data file"""
    try:
        # Get persistence manager
        persistence_manager = get_adapter_persistence_manager(avatar_id)
        
        # Define S3 paths
        training_data_path = f"users/{user_id}/avatars/{avatar_id}/adapters/training_data/"
        metadata_path = f"users/{user_id}/avatars/{avatar_id}/adapters/metadata/"
        
        # Upload file to S3
        file_key = f"{training_data_path}{file.filename}"
        
        # Read file content
        file_content = await file.read()
        
        # Upload to S3
        persistence_manager.s3_client.put_object(
            Bucket=persistence_manager.s3_bucket,
            Key=file_key,
            Body=file_content,
            ContentType=file.content_type or 'application/octet-stream',
            Metadata={
                'user_id': user_id,
                'avatar_id': avatar_id,
                'upload_timestamp': datetime.now().isoformat(),
                'original_filename': file.filename,
                'use_for_training': str(use_for_training)
            }
        )
        
        # Update metadata.json
        metadata_key = f"{metadata_path}metadata.json"
        
        # Get existing metadata or create new
        try:
            metadata_obj = persistence_manager.s3_client.get_object(
                Bucket=persistence_manager.s3_bucket,
                Key=metadata_key
            )
            metadata = json.loads(metadata_obj['Body'].read().decode('utf-8'))
        except:
            metadata = {}
        
        # Update metadata for this file
        metadata[file.filename] = use_for_training
        
        # Upload updated metadata
        persistence_manager.s3_client.put_object(
            Bucket=persistence_manager.s3_bucket,
            Key=metadata_key,
            Body=json.dumps(metadata, indent=2),
            ContentType='application/json'
        )
        
        logger.info(f"Uploaded training file {file.filename} for user {user_id}, avatar {avatar_id}")
        
        return {
            "status": "success",
            "message": f"File {file.filename} uploaded successfully",
            "file_size": len(file_content),
            "use_for_training": use_for_training,
            "s3_key": file_key
        }
        
    except Exception as e:
        logger.error(f"Error uploading training data: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to upload training data: {str(e)}")

@router.get("/{user_id}/{avatar_id}/list")
async def list_training_data(
    user_id: str,
    avatar_id: str,
    training_only: Optional[bool] = Query(None)
) -> List[TrainingDataMetadata]:
    """List training data files with optional filtering"""
    try:
        # Get persistence manager
        persistence_manager = get_adapter_persistence_manager(avatar_id)
        
        # Define S3 paths
        training_data_path = f"users/{user_id}/avatars/{avatar_id}/adapters/training_data/"
        metadata_path = f"users/{user_id}/avatars/{avatar_id}/adapters/metadata/"
        
        # Get metadata
        metadata_key = f"{metadata_path}metadata.json"
        training_metadata = {}
        
        try:
            metadata_obj = persistence_manager.s3_client.get_object(
                Bucket=persistence_manager.s3_bucket,
                Key=metadata_key
            )
            training_metadata = json.loads(metadata_obj['Body'].read().decode('utf-8'))
        except Exception as e:
            logger.warning(f"No training metadata found: {e}")
        
        # List files in training data directory
        response = persistence_manager.s3_client.list_objects_v2(
            Bucket=persistence_manager.s3_bucket,
            Prefix=training_data_path
        )
        
        files_list = []
        
        if 'Contents' in response:
            for obj in response['Contents']:
                # Skip directory-like objects
                if obj['Key'].endswith('/'):
                    continue
                
                filename = os.path.basename(obj['Key'])
                use_for_training = training_metadata.get(filename, False)
                
                # Apply training_only filter
                if training_only is not None:
                    if training_only and not use_for_training:
                        continue
                    elif not training_only and use_for_training:
                        continue
                
                # Get file metadata from S3 object metadata
                try:
                    head_response = persistence_manager.s3_client.head_object(
                        Bucket=persistence_manager.s3_bucket,
                        Key=obj['Key']
                    )
                    file_metadata = head_response.get('Metadata', {})
                except:
                    file_metadata = {}
                
                files_list.append(TrainingDataMetadata(
                    filename=filename,
                    use_for_training=use_for_training,
                    file_size=obj['Size'],
                    last_modified=obj['LastModified'],
                    content_type=file_metadata.get('content-type', 'unknown'),
                    upload_timestamp=file_metadata.get('upload_timestamp'),
                    s3_key=obj['Key']
                ))
        
        logger.info(f"Listed {len(files_list)} training data files for user {user_id}, avatar {avatar_id}")
        
        return files_list
        
    except Exception as e:
        logger.error(f"Error listing training data: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to list training data: {str(e)}")

@router.put("/{user_id}/{avatar_id}/{file_name}/training-flag")
async def update_training_flag(
    user_id: str,
    avatar_id: str,
    file_name: str,
    update: MetadataUpdate
):
    """Update whether a file should be used for training"""
    try:
        # Get persistence manager
        persistence_manager = get_adapter_persistence_manager(avatar_id)
        
        # Define S3 paths
        training_data_path = f"users/{user_id}/avatars/{avatar_id}/adapters/training_data/"
        metadata_path = f"users/{user_id}/avatars/{avatar_id}/adapters/metadata/"
        
        # Check if file exists
        file_key = f"{training_data_path}{file_name}"
        try:
            persistence_manager.s3_client.head_object(
                Bucket=persistence_manager.s3_bucket,
                Key=file_key
            )
        except:
            raise HTTPException(
                status_code=404,
                detail=f"Training data file {file_name} not found"
            )
        
        # Get existing metadata
        metadata_key = f"{metadata_path}metadata.json"
        
        try:
            metadata_obj = persistence_manager.s3_client.get_object(
                Bucket=persistence_manager.s3_bucket,
                Key=metadata_key
            )
            metadata = json.loads(metadata_obj['Body'].read().decode('utf-8'))
        except:
            metadata = {}
        
        # Update metadata for this file
        old_value = metadata.get(file_name, False)
        metadata[file_name] = update.use_for_training
        
        # Upload updated metadata
        persistence_manager.s3_client.put_object(
            Bucket=persistence_manager.s3_bucket,
            Key=metadata_key,
            Body=json.dumps(metadata, indent=2),
            ContentType='application/json'
        )
        
        logger.info(f"Updated training flag for {file_name}: {old_value} -> {update.use_for_training}")
        
        return {
            "status": "success",
            "message": f"Training flag updated for {file_name}",
            "filename": file_name,
            "old_value": old_value,
            "new_value": update.use_for_training
        }
        
    except Exception as e:
        logger.error(f"Error updating training flag: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to update training flag: {str(e)}")

@router.delete("/{user_id}/{avatar_id}/{file_name}")
async def delete_training_file(user_id: str, avatar_id: str, file_name: str):
    """Delete a specific training file"""
    try:
        # Get persistence manager
        persistence_manager = get_adapter_persistence_manager(avatar_id)
        
        # Define S3 paths
        training_data_path = f"users/{user_id}/avatars/{avatar_id}/adapters/training_data/"
        metadata_path = f"users/{user_id}/avatars/{avatar_id}/adapters/metadata/"
        
        # Check if file exists
        file_key = f"{training_data_path}{file_name}"
        try:
            persistence_manager.s3_client.head_object(
                Bucket=persistence_manager.s3_bucket,
                Key=file_key
            )
        except:
            raise HTTPException(
                status_code=404,
                detail=f"Training data file {file_name} not found"
            )
        
        # Delete the file
        persistence_manager.s3_client.delete_object(
            Bucket=persistence_manager.s3_bucket,
            Key=file_key
        )
        
        # Update metadata by removing the file entry
        metadata_key = f"{metadata_path}metadata.json"
        
        try:
            metadata_obj = persistence_manager.s3_client.get_object(
                Bucket=persistence_manager.s3_bucket,
                Key=metadata_key
            )
            metadata = json.loads(metadata_obj['Body'].read().decode('utf-8'))
            
            # Remove file from metadata
            if file_name in metadata:
                del metadata[file_name]
                
                # Upload updated metadata
                persistence_manager.s3_client.put_object(
                    Bucket=persistence_manager.s3_bucket,
                    Key=metadata_key,
                    Body=json.dumps(metadata, indent=2),
                    ContentType='application/json'
                )
        except Exception as e:
            logger.warning(f"Could not update metadata after file deletion: {e}")
        
        logger.info(f"Deleted training file {file_name} for user {user_id}, avatar {avatar_id}")
        
        return {
            "status": "success",
            "message": f"Training file {file_name} deleted successfully",
            "filename": file_name
        }
        
    except Exception as e:
        logger.error(f"Error deleting training file: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to delete training file: {str(e)}")

@router.get("/{user_id}/{avatar_id}/download/{file_name}")
async def download_training_file(user_id: str, avatar_id: str, file_name: str):
    """Download a specific training data file"""
    try:
        # Get persistence manager
        persistence_manager = get_adapter_persistence_manager(avatar_id)
        
        # Define S3 path
        training_data_path = f"users/{user_id}/avatars/{avatar_id}/adapters/training_data/"
        file_key = f"{training_data_path}{file_name}"
        
        # Check if file exists
        try:
            head_response = persistence_manager.s3_client.head_object(
                Bucket=persistence_manager.s3_bucket,
                Key=file_key
            )
        except:
            raise HTTPException(
                status_code=404,
                detail=f"Training data file {file_name} not found"
            )
        
        # Generate presigned URL for download
        try:
            download_url = persistence_manager.s3_client.generate_presigned_url(
                'get_object',
                Params={'Bucket': persistence_manager.s3_bucket, 'Key': file_key},
                ExpiresIn=3600  # 1 hour
            )
            
            return {
                "status": "success",
                "filename": file_name,
                "download_url": download_url,
                "expires_in": 3600,
                "file_size": head_response['ContentLength'],
                "last_modified": head_response['LastModified'].isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error generating presigned URL: {e}")
            raise HTTPException(
                status_code=500,
                detail="Failed to generate download URL"
            )
        
    except Exception as e:
        logger.error(f"Error downloading training file: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to download training file: {str(e)}")

@router.get("/{user_id}/{avatar_id}/metadata")
async def get_training_metadata(user_id: str, avatar_id: str):
    """Get the complete training metadata for an avatar"""
    try:
        # Get persistence manager
        persistence_manager = get_adapter_persistence_manager(avatar_id)
        
        # Define S3 path
        metadata_path = f"users/{user_id}/avatars/{avatar_id}/adapters/metadata/"
        metadata_key = f"{metadata_path}metadata.json"
        
        try:
            metadata_obj = persistence_manager.s3_client.get_object(
                Bucket=persistence_manager.s3_bucket,
                Key=metadata_key
            )
            metadata = json.loads(metadata_obj['Body'].read().decode('utf-8'))
            
            # Count files by training status
            training_files = sum(1 for use_training in metadata.values() if use_training)
            non_training_files = len(metadata) - training_files
            
            return {
                "status": "found",
                "user_id": user_id,
                "avatar_id": avatar_id,
                "metadata": metadata,
                "summary": {
                    "total_files": len(metadata),
                    "training_files": training_files,
                    "non_training_files": non_training_files
                }
            }
            
        except Exception as e:
            if "404" in str(e) or "Not Found" in str(e):
                return {
                    "status": "not_found",
                    "user_id": user_id,
                    "avatar_id": avatar_id,
                    "message": "No training metadata found",
                    "metadata": {},
                    "summary": {
                        "total_files": 0,
                        "training_files": 0,
                        "non_training_files": 0
                    }
                }
            else:
                raise
        
    except Exception as e:
        logger.error(f"Error getting training metadata: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get training metadata: {str(e)}")

@router.delete("/{user_id}/{avatar_id}/non-training-files")
async def delete_non_training_files(user_id: str, avatar_id: str):
    """Delete all files not marked for training"""
    try:
        # Get persistence manager
        persistence_manager = get_adapter_persistence_manager(avatar_id)
        
        # Define S3 paths
        training_data_path = f"users/{user_id}/avatars/{avatar_id}/adapters/training_data/"
        metadata_path = f"users/{user_id}/avatars/{avatar_id}/adapters/metadata/"
        
        # Get metadata
        metadata_key = f"{metadata_path}metadata.json"
        training_metadata = {}
        
        try:
            metadata_obj = persistence_manager.s3_client.get_object(
                Bucket=persistence_manager.s3_bucket,
                Key=metadata_key
            )
            training_metadata = json.loads(metadata_obj['Body'].read().decode('utf-8'))
        except Exception as e:
            logger.warning(f"No training metadata found: {e}")
            return {
                "status": "success",
                "message": "No metadata found, no files to delete",
                "deleted_files": []
            }
        
        # List files in training data directory
        response = persistence_manager.s3_client.list_objects_v2(
            Bucket=persistence_manager.s3_bucket,
            Prefix=training_data_path
        )
        
        files_to_delete = []
        deleted_files = []
        
        if 'Contents' in response:
            for obj in response['Contents']:
                # Skip directory-like objects
                if obj['Key'].endswith('/'):
                    continue
                
                filename = os.path.basename(obj['Key'])
                use_for_training = training_metadata.get(filename, False)
                
                # If not marked for training, mark for deletion
                if not use_for_training:
                    files_to_delete.append({'Key': obj['Key']})
                    deleted_files.append(filename)
        
        # Delete files
        if files_to_delete:
            persistence_manager.s3_client.delete_objects(
                Bucket=persistence_manager.s3_bucket,
                Delete={'Objects': files_to_delete}
            )
            
            # Update metadata by removing deleted files
            updated_metadata = {
                filename: use_for_training 
                for filename, use_for_training in training_metadata.items()
                if filename not in deleted_files
            }
            
            # Upload updated metadata
            persistence_manager.s3_client.put_object(
                Bucket=persistence_manager.s3_bucket,
                Key=metadata_key,
                Body=json.dumps(updated_metadata, indent=2),
                ContentType='application/json'
            )
        
        logger.info(f"Deleted {len(deleted_files)} non-training files for user {user_id}, avatar {avatar_id}")
        
        return {
            "status": "success",
            "message": f"Deleted {len(deleted_files)} files not marked for training",
            "deleted_files": deleted_files
        }
        
    except Exception as e:
        logger.error(f"Error deleting non-training files: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to delete non-training files: {str(e)}")


---


# classes/AdapterPersistenceManager.py

from core.logging import logger
import tempfile
import zipfile
from typing import Dict, Any, List
from datetime import datetime
import json

from fastapi import HTTPException
from botocore.exceptions import ClientError
import os
class AdapterPersistenceManager:
    """Manages persistence operations for LoRA adapters and training data"""
    
    def __init__(self, s3_client, settings, user_id: str, avatar_id: str):
        self.s3_client = s3_client
        self.settings = settings
        self.user_id = user_id
        self.avatar_id = avatar_id
        self.s3_bucket = settings.s3_bucket_name
        
    def _get_s3_adapter_path(self) -> str:
        """Get S3 path for adapters"""
        return f"users/{self.user_id}/avatars/{self.avatar_id}/adapters/"
    
    def _get_s3_training_data_path(self) -> str:
        """Get S3 path for training data"""
        return f"users/{self.user_id}/avatars/{self.avatar_id}/adapters/training_data/"
    
    def _get_s3_metadata_path(self) -> str:
        """Get S3 path for adapter metadata"""
        return f"users/{self.user_id}/avatars/{self.avatar_id}/adapters/metadata/"
    
    async def backup_adapters_to_s3(self, local_adapter_path: str) -> Dict[str, Any]:
        """Backup adapter files to S3"""
        try:
            if not os.path.exists(local_adapter_path):
                raise HTTPException(
                    status_code=404,
                    detail=f"Local adapter path not found: {local_adapter_path}"
                )
            
            # Create zip file of adapters
            with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as temp_file:
                with zipfile.ZipFile(temp_file.name, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for root, dirs, files in os.walk(local_adapter_path):
                        for file in files:
                            file_path = os.path.join(root, file)
                            arcname = os.path.relpath(file_path, local_adapter_path)
                            zipf.write(file_path, arcname)
                
                # Upload to S3
                s3_key = f"{self._get_s3_adapter_path()}adapter_backup.zip"
                self.s3_client.upload_file(temp_file.name, self.s3_bucket, s3_key)
                
                # Create metadata
                metadata = {
                    "backup_type": "adapters",
                    "user_id": self.user_id,
                    "avatar_id": self.avatar_id,
                    "backup_timestamp": datetime.now().isoformat(),
                    "file_count": sum([len(files) for _, _, files in os.walk(local_adapter_path)]),
                    "backup_size_bytes": os.path.getsize(temp_file.name)
                }
                
                # Upload metadata
                metadata_key = f"{self._get_s3_adapter_path()}backup_metadata.json"
                self.s3_client.put_object(
                    Bucket=self.s3_bucket,
                    Key=metadata_key,
                    Body=json.dumps(metadata, indent=2),
                    ContentType='application/json'
                )
                
                # Cleanup temp file
                os.unlink(temp_file.name)
                
                return metadata
                
        except Exception as e:
            logger.error(f"Error backing up adapters: {e}")
            raise
    
    async def backup_training_data_to_s3(self, local_training_data_path: str) -> Dict[str, Any]:
        """Backup training data to S3"""
        try:
            if not os.path.exists(local_training_data_path):
                raise HTTPException(
                    status_code=404,
                    detail=f"Local training data path not found: {local_training_data_path}"
                )
            
            # Create zip file of training data
            with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as temp_file:
                with zipfile.ZipFile(temp_file.name, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for root, dirs, files in os.walk(local_training_data_path):
                        for file in files:
                            file_path = os.path.join(root, file)
                            arcname = os.path.relpath(file_path, local_training_data_path)
                            zipf.write(file_path, arcname)
                
                # Upload to S3
                s3_key = f"{self._get_s3_training_data_path()}training_data_backup.zip"
                self.s3_client.upload_file(temp_file.name, self.s3_bucket, s3_key)
                
                # Create metadata
                metadata = {
                    "backup_type": "training_data",
                    "user_id": self.user_id,
                    "avatar_id": self.avatar_id,
                    "backup_timestamp": datetime.now().isoformat(),
                    "file_count": sum([len(files) for _, _, files in os.walk(local_training_data_path)]),
                    "backup_size_bytes": os.path.getsize(temp_file.name)
                }
                
                # Upload metadata
                metadata_key = f"{self._get_s3_training_data_path()}backup_metadata.json"
                self.s3_client.put_object(
                    Bucket=self.s3_bucket,
                    Key=metadata_key,
                    Body=json.dumps(metadata, indent=2),
                    ContentType='application/json'
                )
                
                # Cleanup temp file
                os.unlink(temp_file.name)
                
                return metadata
                
        except Exception as e:
            logger.error(f"Error backing up training data: {e}")
            raise
    
    async def restore_adapters_from_s3(self, local_adapter_path: str) -> None:
        """Restore adapter files from S3"""
        try:
            s3_key = f"{self._get_s3_adapter_path()}adapter_backup.zip"
            
            # Check if backup exists
            try:
                self.s3_client.head_object(Bucket=self.s3_bucket, Key=s3_key)
            except ClientError as e:
                if e.response['Error']['Code'] == '404':
                    raise HTTPException(
                        status_code=404,
                        detail=f"No adapter backup found for user {self.user_id}, avatar {self.avatar_id}"
                    )
                raise
            
            # Create local directory if it doesn't exist
            os.makedirs(local_adapter_path, exist_ok=True)
            
            # Download and extract
            with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as temp_file:
                self.s3_client.download_file(self.s3_bucket, s3_key, temp_file.name)
                
                with zipfile.ZipFile(temp_file.name, 'r') as zipf:
                    zipf.extractall(local_adapter_path)
                
                # Cleanup temp file
                os.unlink(temp_file.name)
                
        except Exception as e:
            logger.error(f"Error restoring adapters: {e}")
            raise
    
    async def restore_training_data_from_s3(self, local_training_data_path: str) -> None:
        """Restore training data from S3"""
        try:
            s3_key = f"{self._get_s3_training_data_path()}training_data_backup.zip"
            
            # Check if backup exists
            try:
                self.s3_client.head_object(Bucket=self.s3_bucket, Key=s3_key)
            except ClientError as e:
                if e.response['Error']['Code'] == '404':
                    raise HTTPException(
                        status_code=404,
                        detail=f"No training data backup found for user {self.user_id}, avatar {self.avatar_id}"
                    )
                raise
            
            # Create local directory if it doesn't exist
            os.makedirs(local_training_data_path, exist_ok=True)
            
            # Download and extract
            with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as temp_file:
                self.s3_client.download_file(self.s3_bucket, s3_key, temp_file.name)
                
                with zipfile.ZipFile(temp_file.name, 'r') as zipf:
                    zipf.extractall(local_training_data_path)
                
                # Cleanup temp file
                os.unlink(temp_file.name)
                
        except Exception as e:
            logger.error(f"Error restoring training data: {e}")
            raise
    
    async def list_adapter_backups(self) -> List[Dict[str, Any]]:
        """List available adapter backups"""
        backups = []
        
        try:
            # List adapter backups
            adapter_prefix = self._get_s3_adapter_path()
            response = self.s3_client.list_objects_v2(
                Bucket=self.s3_bucket,
                Prefix=adapter_prefix
            )
            
            if 'Contents' in response:
                for obj in response['Contents']:
                    if obj['Key'].endswith('adapter_backup.zip'):
                        # Try to get metadata
                        metadata_key = obj['Key'].replace('adapter_backup.zip', 'backup_metadata.json')
                        metadata = {}
                        try:
                            metadata_obj = self.s3_client.get_object(Bucket=self.s3_bucket, Key=metadata_key)
                            metadata = json.loads(metadata_obj['Body'].read().decode('utf-8'))
                        except:
                            pass
                        
                        backups.append({
                            "type": "adapters",
                            "key": obj['Key'],
                            "size": obj['Size'],
                            "last_modified": obj['LastModified'].isoformat(),
                            "metadata": metadata
                        })
            
            # List training data backups
            training_prefix = self._get_s3_training_data_path()
            response = self.s3_client.list_objects_v2(
                Bucket=self.s3_bucket,
                Prefix=training_prefix
            )
            
            if 'Contents' in response:
                for obj in response['Contents']:
                    if obj['Key'].endswith('training_data_backup.zip'):
                        # Try to get metadata
                        metadata_key = obj['Key'].replace('training_data_backup.zip', 'backup_metadata.json')
                        metadata = {}
                        try:
                            metadata_obj = self.s3_client.get_object(Bucket=self.s3_bucket, Key=metadata_key)
                            metadata = json.loads(metadata_obj['Body'].read().decode('utf-8'))
                        except:
                            pass
                        
                        backups.append({
                            "type": "training_data",
                            "key": obj['Key'],
                            "size": obj['Size'],
                            "last_modified": obj['LastModified'].isoformat(),
                            "metadata": metadata
                        })
            
            return backups
            
        except Exception as e:
            logger.error(f"Error listing adapter backups: {e}")
            raise

---

"""
LoRA Manager for handling LoRA adapter operations
"""

import os
import json
import tempfile
from typing import Dict, Any, Optional, List
from datetime import datetime

from core.logging import logger

class LoRAManager:
    """Manages LoRA adapter creation, training, and deployment"""
    
    def __init__(self):
        self.default_config = {
            "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"],
            "r": 16,
            "lora_alpha": 32,
            "lora_dropout": 0.1,
            "bias": "none",
            "task_type": "CAUSAL_LM"
        }
    
    def create_adapter_config(self, 
                            user_id: str, 
                            avatar_id: str, 
                            adapter_name: str = "default",
                            custom_config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Create a new adapter configuration"""
        
        config = self.default_config.copy()
        if custom_config:
            config.update(custom_config)
        
        adapter_config = {
            "adapter_name": adapter_name,
            "user_id": user_id,
            "avatar_id": avatar_id,
            "created_at": datetime.now().isoformat(),
            "version": "1.0.0",
            "status": "untrained",
            "lora_config": config,
            "training_history": [],
            "model_info": {
                "base_model": "microsoft/DialoGPT-medium",  # Default base model
                "model_type": "gpt2"
            }
        }
        
        logger.info(f"Created adapter config for {user_id}/{avatar_id}")
        return adapter_config
    
    def validate_adapter_structure(self, adapter_path: str) -> bool:
        """Validate that an adapter has the correct structure"""
        required_files = [
            "adapter_config.json",
            "adapter_model.bin"
        ]
        
        if not os.path.exists(adapter_path):
            return False
        
        for required_file in required_files:
            file_path = os.path.join(adapter_path, required_file)
            if not os.path.exists(file_path):
                logger.warning(f"Missing required file: {required_file}")
                return False
        
        return True
    
    def create_empty_adapter(self, adapter_path: str, config: Dict[str, Any]) -> None:
        """Create an empty adapter structure"""
        os.makedirs(adapter_path, exist_ok=True)
        
        # Create adapter config file
        config_path = os.path.join(adapter_path, "adapter_config.json")
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=2)
        
        # Create empty adapter model file (placeholder)
        model_path = os.path.join(adapter_path, "adapter_model.bin")
        with open(model_path, 'wb') as f:
            f.write(b"")  # Empty placeholder
        
        # Create LoRA specific config
        lora_config_path = os.path.join(adapter_path, "adapter_config.json")
        lora_config = {
            "peft_type": "LORA",
            "auto_mapping": None,
            "base_model_name_or_path": config.get("model_info", {}).get("base_model", ""),
            "bias": config["lora_config"]["bias"],
            "fan_in_fan_out": False,
            "inference_mode": True,
            "init_lora_weights": True,
            "layers_pattern": None,
            "layers_to_transform": None,
            "lora_alpha": config["lora_config"]["lora_alpha"],
            "lora_dropout": config["lora_config"]["lora_dropout"],
            "modules_to_save": None,
            "r": config["lora_config"]["r"],
            "revision": None,
            "target_modules": config["lora_config"]["target_modules"],
            "task_type": config["lora_config"]["task_type"]
        }
        
        with open(lora_config_path, 'w') as f:
            json.dump(lora_config, f, indent=2)
        
        logger.info(f"Created empty adapter structure at {adapter_path}")
    
    def load_adapter_config(self, adapter_path: str) -> Optional[Dict[str, Any]]:
        """Load adapter configuration from path"""
        config_path = os.path.join(adapter_path, "adapter_config.json")
        
        if not os.path.exists(config_path):
            logger.warning(f"No adapter config found at {config_path}")
            return None
        
        try:
            with open(config_path, 'r') as f:
                config = json.load(f)
            return config
        except Exception as e:
            logger.error(f"Error loading adapter config: {e}")
            return None
    
    def update_adapter_status(self, adapter_path: str, status: str, additional_info: Optional[Dict] = None) -> bool:
        """Update adapter status in config"""
        config = self.load_adapter_config(adapter_path)
        if not config:
            return False
        
        config["status"] = status
        config["last_updated"] = datetime.now().isoformat()
        
        if additional_info:
            config.update(additional_info)
        
        config_path = os.path.join(adapter_path, "adapter_config.json")
        try:
            with open(config_path, 'w') as f:
                json.dump(config, f, indent=2)
            return True
        except Exception as e:
            logger.error(f"Error updating adapter config: {e}")
            return False
    
    def get_adapter_info(self, adapter_path: str) -> Optional[Dict[str, Any]]:
        """Get comprehensive adapter information"""
        if not self.validate_adapter_structure(adapter_path):
            return None
        
        config = self.load_adapter_config(adapter_path)
        if not config:
            return None
        
        # Get file sizes
        model_path = os.path.join(adapter_path, "adapter_model.bin")
        model_size = os.path.getsize(model_path) if os.path.exists(model_path) else 0
        
        # Count training files if available
        training_history = config.get("training_history", [])
        
        return {
            "config": config,
            "model_size_bytes": model_size,
            "is_trained": config.get("status") == "trained",
            "training_count": len(training_history),
            "last_training": training_history[-1].get("timestamp") if training_history else None,
            "structure_valid": True
        }
    
    def prepare_for_training(self, adapter_path: str, training_files: List[str]) -> Dict[str, Any]:
        """Prepare adapter for training"""
        config = self.load_adapter_config(adapter_path)
        if not config:
            raise ValueError("Invalid adapter configuration")
        
        # Update status
        self.update_adapter_status(adapter_path, "preparing_for_training", {
            "training_files": training_files,
            "preparation_timestamp": datetime.now().isoformat()
        })
        
        return {
            "adapter_path": adapter_path,
            "config": config,
            "training_files": training_files,
            "status": "ready_for_training"
        }
    
    def finalize_training(self, adapter_path: str, training_result: Dict[str, Any]) -> Dict[str, Any]:
        """Finalize adapter after training"""
        config = self.load_adapter_config(adapter_path)
        if not config:
            raise ValueError("Invalid adapter configuration")
        
        # Add to training history
        training_entry = {
            "timestamp": datetime.now().isoformat(),
            "result": training_result,
            "files_used": training_result.get("training_files", []),
            "training_params": training_result.get("training_params", {})
        }
        
        if "training_history" not in config:
            config["training_history"] = []
        config["training_history"].append(training_entry)
        
        # Update status
        final_status = "trained" if training_result.get("success", False) else "training_failed"
        
        additional_info = {
            "last_training": training_entry["timestamp"],
            "training_result": training_result
        }
        
        self.update_adapter_status(adapter_path, final_status, additional_info)
        
        return {
            "status": final_status,
            "training_entry": training_entry,
            "adapter_path": adapter_path
        }

---

## app/core/config.py
import os
from pydantic_settings import BaseSettings
from typing import Optional

class Settings(BaseSettings):
    # AWS Configuration
    aws_access_key_id: Optional[str] = os.getenv("AWS_ACCESS_KEY_ID")
    aws_secret_access_key: Optional[str] = os.getenv("AWS_SECRET_ACCESS_KEY")
    aws_region: str = os.getenv("AWS_REGION", "us-east-1")
    s3_bucket_name: str = os.getenv("S3_BUCKET_NAME", "lora-adapters-bucket")
    
    # Model Configuration
    base_model_name: str = "meta-llama/Llama-3.2-1B-Instruct"
    
    # API Configuration
    debug: bool = os.getenv("DEBUG", "false").lower() == "true"
    
    USER_ID: str
    
    class Config:
        env_file = ".env"

settings = Settings()


__



## app/db/schema/models.py
from pydantic import BaseModel
from typing import Optional, Dict, List, Any
from datetime import datetime
class TrainingDataMetadata(BaseModel):
    file_key: str
    use_for_training: bool = True
    upload_timestamp: datetime
    file_size: Optional[int] = None
    content_type: Optional[str] = None

class AdapterConfig(BaseModel):
    user_id: str
    avatar_id: str
    adapter_name: str
    created_at: datetime
    last_trained: Optional[datetime] = None
    training_status: str = "not_trained"  # not_trained, training, completed, failed

class TrainingRequest(BaseModel):
    user_id: str
    avatar_id: str
    training_params: Optional[Dict] = None

class S3UploadRequest(BaseModel):
    user_id: str
    avatar_id: str
    file_name: str
    use_for_training: bool = True

class MetadataUpdate(BaseModel):
    use_for_training: bool


### Persistence.py Models

# Response models
class AdapterBackupResponse(BaseModel):
    success: bool
    message: str
    backup_info: Dict[str, Any]

class AdapterRestoreResponse(BaseModel):
    success: bool
    message: str

class AdapterListBackupsResponse(BaseModel):
    backups: List[Dict[str, Any]]
    count: int


###
# Add these model definitions to your existing db/schema/models.py file

from pydantic import BaseModel
from typing import Optional, Dict, Any, List
from datetime import datetime

class AdapterConfig(BaseModel):
    """Configuration for LoRA adapter"""
    user_id: str
    avatar_id: str
    adapter_name: str
    status: str  # "created", "training", "trained", "existing", "untrained", "training_failed"
    created_at: datetime
    s3_path: str
    metadata: Optional[Dict[str, Any]] = None
    last_updated: Optional[datetime] = None

class TrainingRequest(BaseModel):
    """Request model for training operations"""
    user_id: str
    avatar_id: str
    training_params: Optional[Dict[str, Any]] = None
    force_retrain: bool = False

class S3UploadRequest(BaseModel):
    """Request model for S3 uploads"""
    user_id: str
    avatar_id: str
    file_path: str
    content_type: Optional[str] = None
    metadata: Optional[Dict[str, Any]] = None

class MetadataUpdate(BaseModel):
    """Request model for updating metadata"""
    use_for_training: bool

class TrainingDataMetadata(BaseModel):
    """Metadata for training data files"""
    filename: str
    use_for_training: bool
    file_size: int
    last_modified: datetime
    content_type: str
    upload_timestamp: Optional[str] = None
    s3_key: str

class AdapterStatus(BaseModel):
    """Status information for an adapter"""
    user_id: str
    avatar_id: str
    exists: bool
    status: str
    last_training: Optional[datetime] = None
    training_files_count: int = 0
    file_size: Optional[int] = None
    created_at: Optional[datetime] = None

class TrainingResult(BaseModel):
    """Result of a training operation"""
    status: str
    message: str
    training_files_used: List[str]
    training_duration: Optional[float] = None
    model_metrics: Optional[Dict[str, float]] = None
    backup_metadata: Optional[Dict[str, Any]] = None

class FileUploadResponse(BaseModel):
    """Response for file upload operations"""
    status: str
    message: str
    filename: str
    file_size: int
    s3_key: str
    use_for_training: bool

class DeleteResponse(BaseModel):
    """Response for delete operations"""
    status: str
    message: str
    deleted_items: List[str]
    deleted_count: int

class DownloadResponse(BaseModel):
    """Response for download operations"""
    status: str
    filename: str
    download_url: str
    expires_in: int
    file_size: int
    last_modified: str

---

# FastAPI LoRA Adapter Management System
## app/main.py
from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import RedirectResponse, Response
from contextlib import asynccontextmanager
import uvicorn
import boto3

from core.config import settings

# Import your existing routers
from api import adapters, training_data, persistence
from core.logging import logger
from service.persistence_service import (
    s3_client_instance, 
    get_s3_client, 
)


@asynccontextmanager
async def lifespan(app: FastAPI):
    global s3_client_instance
    
    # Startup
    logger.info("Application startup - Initializing S3 and settings")
    
    # Get settings and validate required fields
    # settings are initialized on startup

    user_id = settings.USER_ID
    if not user_id:
        logger.error("USER_ID setting is required")
        raise RuntimeError("USER_ID setting is required")
    
    logger.info(f"Starting Adapter Management API for user: {user_id}")
    logger.info(f"App: LoRA Adapter Management API v1.0.0")
    
    # Validate required settings
    if not settings.s3_bucket_name:
        logger.error("S3_BUCKET_NAME is required")
        raise RuntimeError("S3_BUCKET_NAME environment variable is required")
    
    # Initialize S3 client with error handling
    try:
        s3_client = boto3.client(
            's3',
            aws_access_key_id=settings.aws_access_key_id,
            aws_secret_access_key=settings.aws_secret_access_key,
            region_name=settings.aws_region
        )
        # Test S3 connection
        s3_client.head_bucket(Bucket=settings.s3_bucket_name)
        logger.info(f"S3 connection successful to bucket: {settings.s3_bucket_name}")
        s3_client_instance = s3_client
    except Exception as e:
        logger.error(f"Failed to initialize S3 client: {e}")
        raise RuntimeError(f"S3 initialization failed: {e}")
    
    logger.info(f"Adapter persistence configured for user: {user_id}")
    logger.info(f"S3 bucket: {settings.s3_bucket_name}")
    
    yield
    
    # Shutdown
    logger.info("Application shutdown - Cleaning up resources")
    s3_client_instance = None
    logger.info("Cleanup completed")

app = FastAPI(
    title="LoRA Adapter Management API",
    description="API for managing LoRA adapters with S3 storage",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc",
    lifespan=lifespan
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include routers
app.include_router(adapters.router, prefix="/adapters", tags=["adapters"])
app.include_router(training_data.router, prefix="/training-data", tags=["training-data"])
app.include_router(persistence.router, prefix="/persistence", tags=["persistence"])

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    try:
        s3_client = get_s3_client()
        
        # Test S3 connection
        s3_status = "connected"
        try:
            s3_client.head_bucket(Bucket=settings.s3_bucket_name)
        except Exception:
            s3_status = "disconnected"
        
        return {
            "status": "healthy",
            "s3_status": s3_status,
            "s3_bucket": settings.s3_bucket_name,
            "user_id": settings.USER_ID
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e)
        }

@app.get("/", tags=[" Documentation"])
async def root(request: Request):
    return RedirectResponse(url=f"{request.scope.get('root_path', '')}/docs")
---


#service/persistence_service.py
from fastapi import HTTPException
from classes import AdapterPersistenceManager
from core.logging import logger
from core.config import settings

# Global variables to hold managers
s3_client_instance = None

def get_s3_client():
    """Dependency function that returns the global S3 client instance"""
    if s3_client_instance is None:
        raise HTTPException(500, "S3 client not initialized")
    return s3_client_instance


def get_adapter_persistence_manager(avatar_id: str) -> AdapterPersistenceManager:
    """Get adapter persistence manager instance with dynamic user_id from environment"""
    user_id = settings.USER_ID
    
    if not user_id:
        logger.error("USER_ID setting is required")
        raise HTTPException(
            status_code=500,
            detail="USER_ID environment variable is required but not set"
        )
    
    return AdapterPersistenceManager(
        s3_client=get_s3_client(),
        settings=settings,
        user_id=user_id,
        avatar_id=avatar_id
    )
----

"""
S3 Service for handling S3 operations
"""

import json
import tempfile
from typing import Dict, Any, List, Optional, BinaryIO
from datetime import datetime
import os

from botocore.exceptions import ClientError
from core.logging import logger
from core.config import settings

class S3Service:
    """Service for S3 operations"""
    
    def __init__(self):
        self.bucket_name = settings.s3_bucket_name
        self._s3_client = None
    
    @property
    def s3_client(self):
        """Get S3 client from global instance"""
        from service.persistence_service import get_s3_client
        if self._s3_client is None:
            self._s3_client = get_s3_client()
        return self._s3_client
    
    def upload_file(self, 
                   file_content: bytes, 
                   s3_key: str, 
                   content_type: str = 'application/octet-stream',
                   metadata: Optional[Dict[str, str]] = None) -> Dict[str, Any]:
        """Upload file content to S3"""
        
        try:
            extra_args = {
                'ContentType': content_type
            }
            
            if metadata:
                extra_args['Metadata'] = metadata
            
            self.s3_client.put_object(
                Bucket=self.bucket_name,
                Key=s3_key,
                Body=file_content,
                **extra_args
            )
            
            logger.info(f"Uploaded file to S3: {s3_key}")
            
            return {
                "success": True,
                "s3_key": s3_key,
                "bucket": self.bucket_name,
                "size": len(file_content),
                "upload_time": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Failed to upload file to S3: {e}")
            raise
    
    def download_file(self, s3_key: str) -> bytes:
        """Download file content from S3"""
        
        try:
            response = self.s3_client.get_object(
                Bucket=self.bucket_name,
                Key=s3_key
            )
            
            content = response['Body'].read()
            logger.info(f"Downloaded file from S3: {s3_key}")
            
            return content
            
        except ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchKey':
                raise FileNotFoundError(f"File not found in S3: {s3_key}")
            else:
                logger.error(f"Failed to download file from S3: {e}")
                raise
        except Exception as e:
            logger.error(f"Failed to download file from S3: {e}")
            raise
    
    def delete_file(self, s3_key: str) -> bool:
        """Delete file from S3"""
        
        try:
            self.s3_client.delete_object(
                Bucket=self.bucket_name,
                Key=s3_key
            )
            
            logger.info(f"Deleted file from S3: {s3_key}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to delete file from S3: {e}")
            raise
    
    def delete_files(self, s3_keys: List[str]) -> Dict[str, Any]:
        """Delete multiple files from S3"""
        
        try:
            if not s3_keys:
                return {"deleted": 0, "errors": []}
            
            # Prepare objects for deletion
            objects = [{'Key': key} for key in s3_keys]
            
            response = self.s3_client.delete_objects(
                Bucket=self.bucket_name,
                Delete={'Objects': objects}
            )
            
            deleted = response.get('Deleted', [])
            errors = response.get('Errors', [])
            
            logger.info(f"Deleted {len(deleted)} files from S3")
            
            return {
                "deleted": len(deleted),
                "errors": errors,
                "deleted_keys": [obj['Key'] for obj in deleted]
            }
            
        except Exception as e:
            logger.error(f"Failed to delete files from S3: {e}")
            raise
    
    def list_files(self, 
                  prefix: str, 
                  max_keys: int = 1000) -> List[Dict[str, Any]]:
        """List files in S3 with given prefix"""
        
        try:
            response = self.s3_client.list_objects_v2(
                Bucket=self.bucket_name,
                Prefix=prefix,
                MaxKeys=max_keys
            )
            
            files = []
            
            if 'Contents' in response:
                for obj in response['Contents']:
                    # Skip directory-like objects
                    if obj['Key'].endswith('/'):
                        continue
                    
                    files.append({
                        'key': obj['Key'],
                        'filename': os.path.basename(obj['Key']),
                        'size': obj['Size'],
                        'last_modified': obj['LastModified'],
                        'etag': obj['ETag'].strip('"')
                    })
            
            logger.info(f"Listed {len(files)} files with prefix: {prefix}")
            
            return files
            
        except Exception as e:
            logger.error(f"Failed to list files from S3: {e}")
            raise
    
    def file_exists(self, s3_key: str) -> bool:
        """Check if file exists in S3"""
        
        try:
            self.s3_client.head_object(
                Bucket=self.bucket_name,
                Key=s3_key
            )
            return True
            
        except ClientError as e:
            if e.response['Error']['Code'] == '404':
                return False
            else:
                logger.error(f"Error checking file existence: {e}")
                raise
        except Exception as e:
            logger.error(f"Error checking file existence: {e}")
            raise
    
    def get_file_metadata(self, s3_key: str) -> Dict[str, Any]:
        """Get file metadata from S3"""
        
        try:
            response = self.s3_client.head_object(
                Bucket=self.bucket_name,
                Key=s3_key
            )
            
            return {
                'key': s3_key,
                'filename': os.path.basename(s3_key),
                'size': response['ContentLength'],
                'last_modified': response['LastModified'],
                'content_type': response.get('ContentType', 'unknown'),
                'etag': response['ETag'].strip('"'),
                'metadata': response.get('Metadata', {})
            }
            
        except ClientError as e:
            if e.response['Error']['Code'] == '404':
                raise FileNotFoundError(f"File not found in S3: {s3_key}")
            else:
                logger.error(f"Failed to get file metadata: {e}")
                raise
        except Exception as e:
            logger.error(f"Failed to get file metadata: {e}")
            raise
    
    def generate_presigned_url(self, 
                              s3_key: str, 
                              expiration: int = 3600,
                              method: str = 'get_object') -> str:
        """Generate presigned URL for file access"""
        
        try:
            url = self.s3_client.generate_presigned_url(
                method,
                Params={'Bucket': self.bucket_name, 'Key': s3_key},
                ExpiresIn=expiration
            )
            
            logger.info(f"Generated presigned URL for: {s3_key}")
            return url
            
        except Exception as e:
            logger.error(f"Failed to generate presigned URL: {e}")
            raise
    
    def copy_file(self, source_key: str, destination_key: str) -> Dict[str, Any]:
        """Copy file within S3 bucket"""
        
        try:
            copy_source = {
                'Bucket': self.bucket_name,
                'Key': source_key
            }
            
            self.s3_client.copy_object(
                CopySource=copy_source,
                Bucket=self.bucket_name,
                Key=destination_key
            )
            
            logger.info(f"Copied file in S3: {source_key} -> {destination_key}")
            
            return {
                "success": True,
                "source_key": source_key,
                "destination_key": destination_key,
                "copy_time": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Failed to copy file in S3: {e}")
            raise
    
    def upload_json(self, 
                   data: Dict[str, Any], 
                   s3_key: str,
                   metadata: Optional[Dict[str, str]] = None) -> Dict[str, Any]:
        """Upload JSON data to S3"""
        
        json_content = json.dumps(data, indent=2, default=str).encode('utf-8')
        
        return self.upload_file(
            file_content=json_content,
            s3_key=s3_key,
            content_type='application/json',
            metadata=metadata
        )
    
    def download_json(self, s3_key: str) -> Dict[str, Any]:
        """Download and parse JSON from S3"""
        
        try:
            content = self.download_file(s3_key)
            return json.loads(content.decode('utf-8'))
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse JSON from S3: {e}")
            raise ValueError(f"Invalid JSON content in file: {s3_key}")
        except Exception as e:
            logger.error(f"Failed to download JSON from S3: {e}")
            raise
    
    def create_backup(self, 
                     local_path: str, 
                     s3_prefix: str,
                     include_metadata: bool = True) -> Dict[str, Any]:
        """Create backup of local directory to S3"""
        
        if not os.path.exists(local_path):
            raise ValueError(f"Local path does not exist: {local_path}")
        
        uploaded_files = []
        total_size = 0
        
        try:
            # Walk through local directory
            for root, dirs, files in os.walk(local_path):
                for file in files:
                    local_file_path = os.path.join(root, file)
                    relative_path = os.path.relpath(local_file_path, local_path)
                    s3_key = f"{s3_prefix.rstrip('/')}/{relative_path}".replace('\\', '/')
                    
                    # Read and upload file
                    with open(local_file_path, 'rb') as f:
                        file_content = f.read()
                    
                    file_metadata = None
                    if include_metadata:
                        stat = os.stat(local_file_path)
                        file_metadata = {
                            'original_path': local_file_path,
                            'backup_timestamp': datetime.now().isoformat(),
                            'file_size': str(len(file_content)),
                            'last_modified': datetime.fromtimestamp(stat.st_mtime).isoformat()
                        }
                    
                    result = self.upload_file(
                        file_content=file_content,
                        s3_key=s3_key,
                        metadata=file_metadata
                    )
                    
                    uploaded_files.append({
                        'local_path': local_file_path,
                        'relative_path': relative_path,
                        's3_key': s3_key,
                        'size': len(file_content)
                    })
                    
                    total_size += len(file_content)
            
            # Create backup metadata
            backup_metadata = {
                'backup_type': 'directory',
                'source_path': local_path,
                's3_prefix': s3_prefix,
                'backup_timestamp': datetime.now().isoformat(),
                'file_count': len(uploaded_files),
                'total_size': total_size,
                'files': uploaded_files
            }
            
            # Upload backup metadata
            metadata_key = f"{s3_prefix.rstrip('/')}/backup_metadata.json"
            self.upload_json(backup_metadata, metadata_key)
            
            logger.info(f"Created backup: {len(uploaded_files)} files, {total_size} bytes")
            
            return backup_metadata
            
        except Exception as e:
            logger.error(f"Failed to create backup: {e}")
            raise
    
    def restore_backup(self, 
                      s3_prefix: str, 
                      local_path: str,
                      overwrite: bool = False) -> Dict[str, Any]:
        """Restore backup from S3 to local directory"""
        
        # Create local directory
        os.makedirs(local_path, exist_ok=True)
        
        try:
            # Get backup metadata
            metadata_key = f"{s3_prefix.rstrip('/')}/backup_metadata.json"
            backup_metadata = self.download_json(metadata_key)
            
            restored_files = []
            
            for file_info in backup_metadata.get('files', []):
                s3_key = file_info['s3_key']
                relative_path = file_info['relative_path']
                local_file_path = os.path.join(local_path, relative_path)
                
                # Create directory if needed
                local_dir = os.path.dirname(local_file_path)
                if local_dir:
                    os.makedirs(local_dir, exist_ok=True)
                
                # Check if file exists and handle overwrite
                if os.path.exists(local_file_path) and not overwrite:
                    logger.warning(f"Skipping existing file: {local_file_path}")
                    continue
                
                # Download and restore file
                file_content = self.download_file(s3_key)
                
                with open(local_file_path, 'wb') as f:
                    f.write(file_content)
                
                restored_files.append({
                    's3_key': s3_key,
                    'local_path': local_file_path,
                    'size': len(file_content)
                })
            
            logger.info(f"Restored backup: {len(restored_files)} files")
            
            return {
                'success': True,
                'restored_files': restored_files,
                'backup_metadata': backup_metadata,
                'restore_timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Failed to restore backup: {e}")
            raise

----

"""
Training Service for LoRA adapters
"""

import os
import json
import time
import asyncio
from typing import Dict, Any, List, Optional
from datetime import datetime
import tempfile

from core.logging import logger

class TrainingService:
    """Service for training LoRA adapters"""
    
    def __init__(self):
        self.default_training_params = {
            "learning_rate": 2e-4,
            "num_epochs": 3,
            "batch_size": 4,
            "gradient_accumulation_steps": 4,
            "warmup_steps": 100,
            "max_seq_length": 512,
            "save_steps": 500,
            "logging_steps": 10
        }
    
    async def train_lora_adapter(self, 
                                adapter_path: str, 
                                training_data_path: str, 
                                training_params: Dict[str, Any]) -> Dict[str, Any]:
        """Train a LoRA adapter with the provided training data"""
        
        start_time = time.time()
        
        try:
            # Validate inputs
            if not os.path.exists(adapter_path):
                raise ValueError(f"Adapter path does not exist: {adapter_path}")
            
            if not os.path.exists(training_data_path):
                raise ValueError(f"Training data path does not exist: {training_data_path}")
            
            # Merge training parameters
            params = self.default_training_params.copy()
            params.update(training_params)
            
            logger.info(f"Starting LoRA adapter training with params: {params}")
            
            # Get training files
            training_files = self._get_training_files(training_data_path)
            if not training_files:
                raise ValueError("No training files found")
            
            logger.info(f"Found {len(training_files)} training files")
            
            # Validate and prepare training data
            prepared_data = await self._prepare_training_data(training_files, params)
            
            # Simulate training process (replace with actual training logic)
            training_result = await self._simulate_training(
                adapter_path, 
                prepared_data, 
                params
            )
            
            end_time = time.time()
            training_duration = end_time - start_time
            
            # Update adapter with training results
            await self._update_adapter_post_training(
                adapter_path, 
                training_result, 
                training_files,
                training_duration
            )
            
            logger.info(f"Training completed in {training_duration:.2f} seconds")
            
            return {
                "success": True,
                "training_duration": training_duration,
                "training_files": [os.path.basename(f) for f in training_files],
                "training_params": params,
                "model_metrics": training_result.get("metrics", {}),
                "final_loss": training_result.get("final_loss", 0.0),
                "steps_completed": training_result.get("steps", 0),
                "message": "Training completed successfully"
            }
            
        except Exception as e:
            logger.error(f"Training failed: {e}")
            
            end_time = time.time()
            training_duration = end_time - start_time
            
            return {
                "success": False,
                "training_duration": training_duration,
                "error": str(e),
                "message": f"Training failed: {str(e)}"
            }
    
    def _get_training_files(self, training_data_path: str) -> List[str]:
        """Get list of training files from directory"""
        training_files = []
        
        if not os.path.exists(training_data_path):
            return training_files
        
        for filename in os.listdir(training_data_path):
            file_path = os.path.join(training_data_path, filename)
            if os.path.isfile(file_path):
                # Filter for text-based training files
                if filename.lower().endswith(('.txt', '.json', '.jsonl', '.csv')):
                    training_files.append(file_path)
        
        return training_files
    
    async def _prepare_training_data(self, training_files: List[str], params: Dict[str, Any]) -> Dict[str, Any]:
        """Prepare and validate training data"""
        
        total_samples = 0
        total_tokens = 0
        file_info = []
        
        for file_path in training_files:
            try:
                file_size = os.path.getsize(file_path)
                
                # Estimate samples and tokens based on file type and size
                if file_path.endswith('.txt'):
                    # Estimate for text files
                    estimated_tokens = file_size // 4  # Rough estimate
                    estimated_samples = max(1, estimated_tokens // params.get('max_seq_length', 512))
                elif file_path.endswith(('.json', '.jsonl')):
                    # Count lines for JSON files
                    with open(file_path, 'r') as f:
                        lines = sum(1 for _ in f)
                    estimated_samples = lines
                    estimated_tokens = lines * params.get('max_seq_length', 512) // 2
                else:
                    estimated_samples = 1
                    estimated_tokens = file_size // 4
                
                file_info.append({
                    "path": file_path,
                    "filename": os.path.basename(file_path),
                    "size": file_size,
                    "estimated_samples": estimated_samples,
                    "estimated_tokens": estimated_tokens
                })
                
                total_samples += estimated_samples
                total_tokens += estimated_tokens
                
            except Exception as e:
                logger.warning(f"Could not process training file {file_path}: {e}")
        
        return {
            "files": file_info,
            "total_samples": total_samples,
            "total_tokens": total_tokens,
            "estimated_steps": max(1, total_samples // params.get('batch_size', 4))
        }
    
    async def _simulate_training(self, 
                                adapter_path: str, 
                                prepared_data: Dict[str, Any], 
                                params: Dict[str, Any]) -> Dict[str, Any]:
        """Simulate the training process (replace with actual training implementation)"""
        
        total_steps = prepared_data["estimated_steps"] * params.get("num_epochs", 3)
        
        logger.info(f"Simulating training for {total_steps} steps")
        
        # Simulate training progress
        metrics_history = []
        
        for step in range(0, total_steps, max(1, total_steps // 10)):
            # Simulate some processing time
            await asyncio.sleep(0.1)
            
            # Simulate decreasing loss
            loss = 2.0 * (1 - step / total_steps) + 0.1
            
            metrics = {
                "step": step,
                "loss": loss,
                "learning_rate": params.get("learning_rate", 2e-4) * (1 - step / total_steps),
                "timestamp": datetime.now().isoformat()
            }
            
            metrics_history.append(metrics)
            
            if step % max(1, total_steps // 5) == 0:
                logger.info(f"Training step {step}/{total_steps}, loss: {loss:.4f}")
        
        # Generate final metrics
        final_metrics = {
            "final_loss": metrics_history[-1]["loss"] if metrics_history else 1.0,
            "steps": total_steps,
            "samples_processed": prepared_data["total_samples"],
            "tokens_processed": prepared_data["total_tokens"],
            "metrics_history": metrics_history[-5:],  # Keep last 5 entries
            "convergence": "good" if metrics_history[-1]["loss"] < 0.5 else "fair"
        }
        
        return {
            "metrics": final_metrics,
            "final_loss": final_metrics["final_loss"],
            "steps": total_steps,
            "status": "completed"
        }
    
    async def _update_adapter_post_training(self, 
                                          adapter_path: str, 
                                          training_result: Dict[str, Any],
                                          training_files: List[str],
                                          training_duration: float) -> None:
        """Update adapter files after training"""
        
        # Update adapter config
        config_path = os.path.join(adapter_path, "adapter_config.json")
        
        if os.path.exists(config_path):
            with open(config_path, 'r') as f:
                config = json.load(f)
            
            # Update training information
            config["status"] = "trained"
            config["last_training"] = datetime.now().isoformat()
            config["training_duration"] = training_duration
            config["training_files_used"] = [os.path.basename(f) for f in training_files]
            
            # Add training metrics
            if "training_metrics" not in config:
                config["training_metrics"] = {}
            
            config["training_metrics"]["latest"] = training_result.get("metrics", {})
            
            # Save updated config
            with open(config_path, 'w') as f:
                json.dump(config, f, indent=2)
        
        # Update adapter model file (simulate trained weights)
        model_path = os.path.join(adapter_path, "adapter_model.bin")
        
        # Create a more substantial model file to simulate trained weights
        model_data = {
            "model_state": "trained",
            "training_timestamp": datetime.now().isoformat(),
            "training_metrics": training_result.get("metrics", {}),
            "model_version": "1.0.0"
        }
        
        # Write binary data (in real implementation, this would be actual model weights)
        with open(model_path, 'wb') as f:
            # Write some dummy data to simulate a trained model
            dummy_weights = json.dumps(model_data).encode() * 100  # Make it larger
            f.write(dummy_weights)
        
        logger.info(f"Updated adapter post-training at {adapter_path}")
    
    async def validate_training_data(self, training_data_path: str) -> Dict[str, Any]:
        """Validate training data format and content"""
        
        if not os.path.exists(training_data_path):
            return {"valid": False, "error": "Training data path does not exist"}
        
        training_files = self._get_training_files(training_data_path)
        
        if not training_files:
            return {"valid": False, "error": "No valid training files found"}
        
        validation_results = []
        total_size = 0
        
        for file_path in training_files:
            file_result = {
                "filename": os.path.basename(file_path),
                "path": file_path,
                "size": os.path.getsize(file_path),
                "valid": True,
                "issues": []
            }
            
            # Check file size
            if file_result["size"] == 0:
                file_result["valid"] = False
                file_result["issues"].append("File is empty")
            elif file_result["size"] > 100 * 1024 * 1024:  # 100MB limit
                file_result["issues"].append("File is very large (>100MB)")
            
            # Try to read and validate file content
            try:
                if file_path.endswith('.txt'):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read(1000)  # Read first 1000 chars
                        if not content.strip():
                            file_result["valid"] = False
                            file_result["issues"].append("File contains no readable text")
                
                elif file_path.endswith(('.json', '.jsonl')):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        first_line = f.readline()
                        try:
                            json.loads(first_line)
                        except json.JSONDecodeError:
                            file_result["issues"].append("Invalid JSON format")
                            
            except Exception as e:
                file_result["valid"] = False
                file_result["issues"].append(f"Could not read file: {str(e)}")
            
            total_size += file_result["size"]
            validation_results.append(file_result)
        
        valid_files = [r for r in validation_results if r["valid"]]
        
        return {
            "valid": len(valid_files) > 0,
            "total_files": len(training_files),
            "valid_files": len(valid_files),
            "total_size": total_size,
            "files": validation_results,
            "summary": {
                "has_valid_files": len(valid_files) > 0,
                "total_size_mb": total_size / (1024 * 1024),
                "recommended_training": len(valid_files) > 0 and total_size > 1024  # At least 1KB
            }
        }
    
    async def get_training_recommendations(self, 
                                         training_data_path: str, 
                                         current_params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Get training parameter recommendations based on data"""
        
        validation = await self.validate_training_data(training_data_path)
        
        if not validation["valid"]:
            return {
                "recommended": False,
                "reason": "No valid training data found",
                "validation": validation
            }
        
        # Analyze data size and recommend parameters
        total_size_mb = validation["summary"]["total_size_mb"]
        valid_files = validation["valid_files"]
        
        recommendations = self.default_training_params.copy()
        
        # Adjust parameters based on data size
        if total_size_mb < 1:  # Very small dataset
            recommendations.update({
                "num_epochs": 5,
                "learning_rate": 1e-4,
                "batch_size": 2,
                "gradient_accumulation_steps": 8
            })
            difficulty = "small_dataset"
            
        elif total_size_mb < 10:  # Small dataset
            recommendations.update({
                "num_epochs": 4,
                "learning_rate": 2e-4,
                "batch_size": 4,
                "gradient_accumulation_steps": 4
            })
            difficulty = "medium_dataset"
            
        elif total_size_mb < 50:  # Medium dataset
            recommendations.update({
                "num_epochs": 3,
                "learning_rate": 3e-4,
                "batch_size": 8,
                "gradient_accumulation_steps": 2
            })
            difficulty = "large_dataset"
            
        else:  # Large dataset
            recommendations.update({
                "num_epochs": 2,
                "learning_rate": 5e-4,
                "batch_size": 16,
                "gradient_accumulation_steps": 1
            })
            difficulty = "very_large_dataset"
        
        # Estimate training time
        estimated_samples = validation["total_size"] // 100  # Rough estimate
        estimated_steps = (estimated_samples * recommendations["num_epochs"]) // recommendations["batch_size"]
        estimated_time_minutes = max(1, estimated_steps // 100)  # Very rough estimate
        
        return {
            "recommended": True,
            "difficulty": difficulty,
            "recommended_params": recommendations,
            "current_params": current_params or {},
            "estimates": {
                "training_steps": estimated_steps,
                "estimated_time_minutes": estimated_time_minutes,
                "estimated_samples": estimated_samples
            },
            "data_analysis": {
                "total_size_mb": total_size_mb,
                "file_count": valid_files,
                "data_quality": "good" if validation["summary"]["recommended_training"] else "limited"
            },
            "validation": validation
        }